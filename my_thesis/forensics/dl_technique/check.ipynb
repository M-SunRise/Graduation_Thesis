{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained backbone:  True\n",
      "Pretrained backbone:  True\n",
      "lr =  [0.001, 0.0001]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit import Transformer\n",
    "from pytorchcv.model_provider import get_model\n",
    "\n",
    "class OriDualEfficientViT(nn.Module):\n",
    "    def __init__(self, channels=1280,\\\n",
    "                 image_size=224,patch_size=7,num_classes=1,dim=1024,\\\n",
    "                 depth=6,heads=8,mlp_dim=2048,\\\n",
    "                 emb_dim=32, dim_head=64,dropout=0.15,emb_dropout=0.15,backbone=\"xception_net\", version=\"cross_attention-spatial-cat\",weight=0.5,freeze=0):  \n",
    "        super(OriDualEfficientViT, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.features_size = {\n",
    "            128: (4, 4),\n",
    "            224: (7, 7),\n",
    "            256: (8, 8)\n",
    "        }\n",
    "        \n",
    "        # \"cross_attention-spatial-cat\": sử dụng cross-attention, cat với spatial vectors output\n",
    "        # \"cross_attention-spatial-add\": sử dụng cross-attention, add với spatial vectors output\n",
    "        # \"cross_attention-freq-cat\": sử dụng cross-attention, cat với freq vectors\n",
    "        # \"cross_attention-freq-add\": sử dụng cross-attention, add với freq vectors\n",
    "        # \"merge-add\": cộng thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        # \"merge-cat\": cat thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        self.version = version\n",
    "        self.weight = weight\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.spatial_extractor = self.get_feature_extractor(freeze=freeze, architecture=backbone, num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(freeze=freeze, architecture=backbone, num_classes=num_classes, in_channels=1)\n",
    "\n",
    "        ############################# Xét 2 stream hiện tại là như nhau\n",
    "        # Kích thước của 1 patch\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "        # Số lượng patches\n",
    "        self.num_patches = int((self.features_size[image_size][0] * self.features_size[image_size][1]) / (self.patch_size * self.patch_size))\n",
    "        # Patch_dim = P^2 * C\n",
    "        self.patch_dim = channels * (self.patch_size ** 2)\n",
    "\n",
    "        # print(\"Num patches: \", self.num_patches)\n",
    "        # print(\"Patch dim: \", self.patch_dim)\n",
    "\n",
    "        # Embed vị trí cho từng patch\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches+1, self.dim))\n",
    "        # self.pos_embedding_1 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_2 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_3 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "\n",
    "        # Đưa flatten vector của feature maps về chiều cố định của vector trong transformer.\n",
    "        # self.patch_to_embedding_1 = nn.Linear(self.patch_dim, self.dim)\n",
    "        # self.patch_to_embedding_2 = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Giảm chiều vector sau concat 2*patch_dim về D:\n",
    "        self.patch_to_embedding_cat = nn.Linear(2*self.patch_dim, self.dim)\n",
    "        self.patch_to_embedding_add = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Thêm 1 embedding vector cho classify token:\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"xception_net\", freeze=0, pretrained=True, num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            if pretrained == \"\":\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b0', pretrained=True, num_classes=num_classes,in_channels = in_channels)\n",
    "            else:\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b7', num_classes=num_classes,in_channels = in_channels)\n",
    "                # Load checkpoint\n",
    "                checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "                state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
    "                # Load weights\n",
    "                extractor.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=False)\n",
    "\n",
    "            if freeze:\n",
    "            # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "                print(\"Here\")\n",
    "                for i in range(0, len(extractor._blocks)):\n",
    "                    for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "                        if i >= len(extractor._blocks) - 3:\n",
    "                            param.requires_grad = True\n",
    "                        else:\n",
    "                            param.requires_grad = False\n",
    "        if architecture == 'xception_net':\n",
    "            xception = get_model(\"xception\", pretrained=bool(pretrained))\n",
    "            extractor = nn.Sequential(*list(xception.children())[:-1])\n",
    "            extractor[0].final_block.pool = nn.Identity()\n",
    "            if in_channels != 3:\n",
    "                extractor[0].init_block.conv1.conv = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "\n",
    "        print(\"Pretrained backbone: \", bool(pretrained))\n",
    "        return extractor\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        print(\"in scale dot.\")\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        print('q after: ', q)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        print(\"attn: \", attn)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "    def cross_attention(self, spatials, ifreqs):\n",
    "        \"\"\"\n",
    "            spatials: (B, N, D) --> Query,\n",
    "            freqs: (B, N, D) --> Key\n",
    "            output: \n",
    "        \"\"\"\n",
    "        print(\"q before scaledot: \", spatials)\n",
    "        print(\"k before scaledot: \", ifreqs)\n",
    "        print(\"v before scaledot: \", ifreqs)\n",
    "        emb_dim = spatials.shape[2]\n",
    "        assert emb_dim == ifreqs.shape[2]\n",
    "        attn_outputs, attn_weights = self.scale_dot(spatials, ifreqs, ifreqs)\n",
    "        return attn_outputs, attn_weights\n",
    "\n",
    "    def extract_feature(self, rgb_imgs, freq_imgs):\n",
    "        if self.backbone == 'efficient_net':\n",
    "            rgb_features = self.spatial_extractor.extract_features(rgb_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "            freq_features = self.freq_extractor.extract_features(freq_imgs)              # shape (batchsize, 1280, 4, 4)\n",
    "        else:\n",
    "            rgb_features = self.spatial_extractor(rgb_imgs)\n",
    "            freq_features = self.freq_extractor(freq_imgs)\n",
    "        return rgb_features, freq_features\n",
    "\n",
    "    def forward(self, spatial_imgs, frequency_imgs):\n",
    "        p = self.patch_size\n",
    "        # Extract features\n",
    "        # print(frequency_imgs)\n",
    "        spatial_features, freq_features = self.extract_feature(spatial_imgs, frequency_imgs)                     # shape (batchsize, 1280, 8, 8)conda\n",
    "        ifreq_features = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_features))) + 1e-10)  # Hơi ảo???\n",
    "        # return spatial_features, freq_features, ifreq_features\n",
    "        # print(ifreq_features.shape)\n",
    "        # assert(ifreq_features.shape == freq_features.shape)\n",
    "        # print(\"Features shape: \", spatial_features.shape, freq_features.shape)\n",
    "\n",
    "        # Flatten to vector:\n",
    "        spatial_vectors = rearrange(spatial_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        freq_vectors = rearrange(freq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        ifreq_vectors = rearrange(ifreq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "\n",
    "        assert self.patch_dim == spatial_vectors.shape[2]\n",
    "        assert self.patch_dim == freq_vectors.shape[2]\n",
    "\n",
    "        embed = None\n",
    "        print(\"1: \", spatial_vectors)\n",
    "        print(\"2: \", ifreq_vectors)\n",
    "        print(\"3: \", ifreq_vectors)\n",
    "        if \"cross_attention\" in self.version:          # Merge using cross-attention  \n",
    "            ########## Patch embedding and add position embedding to each domain:\n",
    "            # spatial_vectors = self.patch_to_embedding_1(spatial_vectors)\n",
    "            # spatial_vectors += self.pos_embedding_1\n",
    "\n",
    "            # freq_vectors = self.patch_to_embedding_2(freq_vectors)\n",
    "            # freq_vectors += self.pos_embedding_2\n",
    "\n",
    "            # ifreq_vectors = self.patch_to_embedding_2(ifreq_vectors)\n",
    "            # ifreq_vectors += self.pos_embedding_2  \n",
    "            # print(\"Step 2 shape: \", spatial_vectors.shape, freq_vectors.shape)  # (batchsize, num_patches, D)\n",
    "            ##########\n",
    "        \n",
    "            # Cal attn weight between ifreq and spatial vectors:\n",
    "            # Cross-attention (spatial-decoder, ifreq-encoder)\n",
    "            attn_outputs, attn_weights = self.cross_attention(spatial_vectors, ifreq_vectors)     # Shape: (), (batchsize, num_patches, num_patches)\n",
    "            print(\"Attn weights: \\n\", attn_outputs, attn_weights)\n",
    "            if \"freq\" in self.version:          # Get attention in frequency domain:\n",
    "                out_attn = torch.bmm(attn_weights, freq_vectors)\n",
    "            elif \"spatial\" in self.version:     # Get attention in spatial domain:\n",
    "                out_attn = torch.bmm(attn_weights, ifreq_vectors)\n",
    "                ### Check correct bmm:\n",
    "                # print(torch.eq(attn_outputs, out_attn))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Concat or add and linear\n",
    "            # print(\"Spatial vectors: \", spatial_vectors.shape)\n",
    "            # print(spatial_vectors)\n",
    "            # print(\"Output attention: \", out_attn.shape)\n",
    "            # print(out_attn)\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * out_attn)\n",
    "                # print(\"Out\", out)\n",
    "                embed = self.patch_to_embedding_add(out)                 # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * out_attn], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                 # Shape: (batchsize, num_patches, 2*patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "        else:   # Merge directly\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * freq_vectors)\n",
    "                embed = self.patch_to_embedding_add(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * freq_vectors], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "        print(\"Fusion out: \\n\", out)\n",
    "        print(\"Embed: \", embed)\n",
    "        # print(\"Embeded shape: \", embed.shape)\n",
    "\n",
    "        # Expand classify token to batchsize and add to patch embeddings:\n",
    "        cls_tokens = self.cls_token.expand(embed.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, embed), dim=1)   # (batchsize, num_patches+1, dim)\n",
    "        x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, in_dim, inner_dim=0, prj_out=False, qkv_embed=True, init_weight=True):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.qkv_embed = qkv_embed\n",
    "        self.init_weight = init_weight\n",
    "        self.to_out = nn.Identity()\n",
    "        if self.qkv_embed:\n",
    "            inner_dim = self.in_dim if inner_dim == 0 else inner_dim\n",
    "            self.to_k = nn.Linear(in_dim, inner_dim, bias=False)\n",
    "            self.to_v = nn.Linear(in_dim, inner_dim, bias = False)\n",
    "            self.to_q = nn.Linear(in_dim, inner_dim, bias = False)\n",
    "            self.to_out = nn.Sequential(\n",
    "                nn.Linear(inner_dim, in_dim),\n",
    "                nn.Dropout(p=0.1)\n",
    "            ) if prj_out else nn.Identity()\n",
    "\n",
    "        if self.init_weight:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        \"\"\"\n",
    "            x ~ rgb_vectors: (b, n, in_dim)\n",
    "            y ~ freq_vectors: (b, n, in_dim)\n",
    "            z ~ freq_vectors: (b, n, in_dim)\n",
    "            Returns:\n",
    "                attn_weight: (b, n, n)\n",
    "                attn_output: (b, n, in_dim)\n",
    "        \"\"\"\n",
    "        if self.qkv_embed:\n",
    "            q = self.to_q(x)\n",
    "            k = self.to_k(y)\n",
    "            v = self.to_v(z)\n",
    "        else:\n",
    "            q, k, v = x, y, z\n",
    "        print(\"x before scaledot: \", q)\n",
    "        print(\"y before scaledot: \", k)\n",
    "        print(\"z before scaledot: \", v)\n",
    "        out, attn = self.scale_dot(q, k, v, dropout_p=0.00)\n",
    "        out = self.to_out(out)\n",
    "        return out, attn\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        print(\"in scale dot.\")\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        print('q after: ', q)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        print(\"attn: \", attn)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "class MyDualEfficientViT(nn.Module):\n",
    "    def __init__(self, \\\n",
    "                image_size=224, num_classes=1, dim=1024,\\\n",
    "                depth=6, heads=8, mlp_dim=2048,\\\n",
    "                dim_head=64, dropout=0.15, emb_dropout=0.15,\\\n",
    "                backbone='xception_net', pretrained=True,\\\n",
    "                normalize_ifft=True,\\\n",
    "                flatten_type='patch',\\\n",
    "                conv_attn=False, ratio=5, qkv_embed=True, init_ca_weight=True, prj_out=False, inner_ca_dim=512, act='none',\\\n",
    "                patch_size=7, position_embed=False, pool='cls',\\\n",
    "                version='ca-fcat-0.5', unfreeze_blocks=-1, \\\n",
    "                init_linear=\"xavier\", init_layernorm=\"normal\", init_conv=\"kaiming\"):  \n",
    "        super(MyDualEfficientViT, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.features_size = {\n",
    "            'efficient_net': (1280, 4, 4),\n",
    "            'xception_net': (2048, 4, 4),\n",
    "        }\n",
    "        self.out_ext_channels = self.features_size[backbone][0]\n",
    "        \n",
    "        self.flatten_type = flatten_type # in ['patch', 'channel']\n",
    "        self.version = version  # in ['ca-rgb_cat-0.5', 'ca-freq_cat-0.5']\n",
    "        self.position_embed = position_embed\n",
    "        self.pool = pool\n",
    "        self.conv_attn = conv_attn\n",
    "        self.activation = self.get_activation(act)\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.rgb_extractor = self.get_feature_extractor(architecture=backbone, pretrained=pretrained, unfreeze_blocks=unfreeze_blocks, num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(architecture=backbone, pretrained=pretrained, unfreeze_blocks=unfreeze_blocks, num_classes=num_classes, in_channels=1)\n",
    "        self.normalize = nn.BatchNorm2d(num_features=self.out_ext_channels) if normalize_ifft else nn.Identity()\n",
    "        ############################# PATCH CONFIG ################################\n",
    "        \n",
    "        if self.flatten_type == 'patch':\n",
    "            # Kích thước của 1 patch\n",
    "            self.patch_size = patch_size\n",
    "            # Số lượng patches\n",
    "            self.num_patches = int((self.features_size[backbone][1] * self.features_size[backbone][2]) / (self.patch_size * self.patch_size))\n",
    "            # Patch_dim = P^2 * C\n",
    "            self.patch_dim = self.out_ext_channels//ratio * (self.patch_size ** 2)\n",
    "\n",
    "        ############################# CROSS ATTENTION #############################\n",
    "        if self.flatten_type == 'patch':\n",
    "            self.in_dim = self.patch_dim\n",
    "        else:\n",
    "            self.in_dim = int(self.features_size[backbone][1] * self.features_size[backbone][2])\n",
    "        if self.conv_attn:\n",
    "            self.query_conv = nn.Conv2d(in_channels=self.out_ext_channels, out_channels=self.out_ext_channels//ratio, kernel_size=1)\n",
    "            self.key_conv = nn.Conv2d(in_channels=self.out_ext_channels, out_channels=self.out_ext_channels//ratio, kernel_size=1)\n",
    "            self.value_conv = nn.Conv2d(in_channels=self.out_ext_channels, out_channels=self.out_ext_channels//ratio, kernel_size=1)\n",
    "\n",
    "        self.CA = CrossAttention(in_dim=self.in_dim, inner_dim=inner_ca_dim, prj_out=prj_out, qkv_embed=qkv_embed, init_weight=init_ca_weight)\n",
    "\n",
    "        ############################# VIT #########################################\n",
    "        # Number of vectors:\n",
    "        self.num_vecs = self.num_patches if self.flatten_type == 'patch' else self.out_ext_channels//ratio\n",
    "        # Embed vị trí cho từng vectors (nếu chia theo patch):\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_vecs+1, self.dim))\n",
    "        # Giảm chiều vector sau concat 2*patch_dim về D:\n",
    "        if 'cat' in self.version:\n",
    "            self.embedding = nn.Linear(2 * self.in_dim, self.dim)\n",
    "        else:\n",
    "            self.embedding = nn.Linear(self.in_dim, self.dim)\n",
    "\n",
    "        # Thêm 1 embedding vector cho classify token:\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init_linear, self.init_layernorm, self.init_conv = init_linear, init_layernorm, init_conv\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    def get_activation(self, act):\n",
    "        if act == 'relu':\n",
    "            activation = nn.ReLU(inplace=True)\n",
    "        elif act == 'tanh':\n",
    "            activation = nn.Tanh()\n",
    "        else:\n",
    "            activation = None\n",
    "        return activation\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", unfreeze_blocks=-1, pretrained=False, num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels, pretrained=bool(pretrained))\n",
    "            if unfreeze_blocks != -1:\n",
    "                # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "                for i in range(0, len(extractor._blocks)):\n",
    "                    for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "                        if i >= len(extractor._blocks) - unfreeze_blocks:\n",
    "                            param.requires_grad = True\n",
    "                        else:\n",
    "                            param.requires_grad = False\n",
    "        \n",
    "        if architecture == 'xception_net':\n",
    "            xception = get_model(\"xception\", pretrained=bool(pretrained))\n",
    "            extractor = nn.Sequential(*list(xception.children())[:-1])\n",
    "            extractor[0].final_block.pool = nn.Identity()\n",
    "            if in_channels != 3:\n",
    "                extractor[0].init_block.conv1.conv = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "\n",
    "            if unfreeze_blocks != -1:\n",
    "                blocks = len(extractor[0].children())\n",
    "                print(\"Number of blocks in xception: \", len(blocks))\n",
    "                for i, block in enumerate(extractor[0].children()):\n",
    "                    if i >= blocks - unfreeze_blocks:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    else:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = False\n",
    "        print(\"Pretrained backbone: \", bool(pretrained))\n",
    "        # if not pretrained:\n",
    "        #     self.init_conv_weight(extractor)\n",
    "        return extractor\n",
    "\n",
    "    def flatten_to_vectors(self, feature):\n",
    "        vectors = None\n",
    "        if self.flatten_type == 'patch':\n",
    "            vectors = rearrange(feature, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "        elif self.flatten_type == 'channel':\n",
    "            vectors = rearrange(feature, 'b c h w -> b c (h w)')\n",
    "        else:\n",
    "            pass\n",
    "        return vectors\n",
    "\n",
    "    def ifft(self, freq_feature):\n",
    "        ifreq_feature = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_feature))) + 1e-10)  # Hơi ảo???\n",
    "        ifreq_feature = self.normalize(ifreq_feature)\n",
    "        return ifreq_feature\n",
    "\n",
    "    def fusion(self, rgb, out_attn):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            rgb --      b, n, d\n",
    "            out_attn -- b, n, d\n",
    "        \"\"\"\n",
    "        weight = float(self.version.split('-')[-1])\n",
    "        if 'cat' in self.version:\n",
    "            out = torch.cat([rgb, weight * out_attn], dim=2)\n",
    "        elif 'add' in self.version:\n",
    "            out = torch.add(rgb, weight * out_attn)\n",
    "        return out\n",
    "\n",
    "    def extract_feature(self, rgb_imgs, freq_imgs):\n",
    "        if self.backbone == 'efficient_net':\n",
    "            rgb_features = self.rgb_extractor.extract_features(rgb_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "            freq_features = self.freq_extractor.extract_features(freq_imgs)              # shape (batchsize, 1280, 4, 4)\n",
    "        else:\n",
    "            rgb_features = self.rgb_extractor(rgb_imgs)\n",
    "            freq_features = self.freq_extractor(freq_imgs)\n",
    "        return rgb_features, freq_features\n",
    "\n",
    "    def forward(self, rgb_imgs, freq_imgs):\n",
    "        rgb_features, freq_features = self.extract_feature(rgb_imgs, freq_imgs)\n",
    "        ifreq_features = self.ifft(freq_features)\n",
    "        # return rgb_features, freq_features, ifreq_features\n",
    "        # print(\"Features shape: \", rgb_features.shape, freq_features.shape, ifreq_features.shape)\n",
    "\n",
    "        # Turn to q, k, v if use conv-attention, and then flatten to vector:\n",
    "        if self.conv_attn:\n",
    "            rgb_query = self.query_conv(rgb_features)\n",
    "            freq_value = self.value_conv(freq_features)\n",
    "            ifreq_key = self.key_conv(ifreq_features)\n",
    "            ifreq_value = self.value_conv(ifreq_features)\n",
    "        else:\n",
    "            rgb_query = rgb_features\n",
    "            freq_value = freq_features\n",
    "            ifreq_key = ifreq_features\n",
    "            ifreq_value = ifreq_features\n",
    "        # print(\"Q K V shape: \", rgb_query.shape, freq_value.shape, ifreq_key.shape, ifreq_value.shape)\n",
    "        rgb_query_vectors = self.flatten_to_vectors(rgb_query)\n",
    "        freq_value_vectors = self.flatten_to_vectors(freq_value)\n",
    "        ifreq_key_vectors = self.flatten_to_vectors(ifreq_key)\n",
    "        ifreq_value_vectors = self.flatten_to_vectors(ifreq_value)\n",
    "        # print(\"Vectors shape: \", rgb_query_vectors.shape, freq_value_vectors.shape, ifreq_key_vectors.shape, ifreq_value_vectors.shape)\n",
    "\n",
    "        ##### Cross attention and fusion:\n",
    "        # print(\"1: \", rgb_query_vectors)\n",
    "        # print(\"2: \", ifreq_key_vectors)\n",
    "        # print(\"3: \", ifreq_value_vectors)\n",
    "        out, attn_weight = self.CA(rgb_query_vectors, ifreq_key_vectors, ifreq_value_vectors)\n",
    "\n",
    "        attn_out = torch.bmm(attn_weight, freq_value_vectors)\n",
    "        fusion_out = self.fusion(rgb_query_vectors, attn_out)\n",
    "        if self.activation is not None:\n",
    "            fusion_out = self.activation(fusion_out)\n",
    "        # print(\"Fusion shape: \", fusion_out.shape)\n",
    "        embed = self.embedding(fusion_out)\n",
    "        print(\"Fusion: \\n\", fusion_out)\n",
    "        print(\"embed: \", embed)\n",
    "        # print(\"Inner ViT shape: \", embed.shape)\n",
    "\n",
    "        ##### Forward to ViT\n",
    "        # Expand classify token to batchsize and add to patch embeddings:\n",
    "        cls_tokens = self.cls_token.expand(embed.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, embed), dim=1)   # (batchsize, in_dim+1, dim)\n",
    "        if self.position_embed:\n",
    "            x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x.mean(dim = 1) if self.pool == 'mean' else x[:, 0])\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "from torchsummary import summary\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(0)\n",
    "    x = torch.ones(1, 3, 128, 128)\n",
    "    y = torch.ones(1, 1, 128, 128)\n",
    "    model_1 = MyDualEfficientViT( image_size=128, num_classes=1, dim=1024,\\\n",
    "                                depth=6, heads=8, mlp_dim=2048,\\\n",
    "                                dim_head=64, dropout=0.15, emb_dropout=0.15,\\\n",
    "                                backbone='xception_net', pretrained=True,\\\n",
    "                                normalize_ifft=False,\\\n",
    "                                flatten_type='patch',\\\n",
    "                                conv_attn=False, ratio=1, qkv_embed=False, inner_ca_dim=0, init_ca_weight=False, prj_out=False, act='none',\\\n",
    "                                patch_size=1, position_embed=True, pool='cls',\\\n",
    "                                version='ca-fadd-0.8', unfreeze_blocks=-1)\n",
    "    # model_1.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     print(\"Model 1\")\n",
    "    #     rgb_1, freq_1, ifreq_1 = model_1(x, y)\n",
    "    #     print(\"************************************* RGB 1 ***\")\n",
    "    #     # print(freq_1)\n",
    "    #     import numpy as np\n",
    "\n",
    "\n",
    "    # model_2 = OriDualEfficientViT(channels=2048, image_size=128, num_classes=1, dim=1024,\\\n",
    "    #                             depth=6, heads=8, mlp_dim=2048,\\\n",
    "    #                             dim_head=64, dropout=0.15, emb_dropout=0.15,\\\n",
    "    #                             patch_size=1, backbone=\"xception_net\", version='cross_attention-freq-add', weight=0.8, freeze=0)\n",
    "    # model_2.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     print(\"Model 2\")\n",
    "    #     rgb_2, freq_2, ifreq_2 = model_2(x, y)\n",
    "    #     print(\"************************************* RGB 2 ***\")\n",
    "    #     print(freq_2)\n",
    "\n",
    "    # if torch.equal(rgb_1, rgb_2):\n",
    "    #     print(\"Equal rgb\")\n",
    "    # if torch.equal(freq_1, freq_2):\n",
    "    #     print(\"Equal freq\")\n",
    "    # if torch.equal(ifreq_1, ifreq_2):\n",
    "    #     print(\"Equal ifreq\")\n",
    "    extractor = []\n",
    "    rest = []\n",
    "    for name, param in model_1.named_parameters():\n",
    "        if 'extractor' in name:\n",
    "            extractor.append(param)\n",
    "        else:\n",
    "            rest.append(param)\n",
    "\n",
    "    from torch.optim import Adam\n",
    "    optim = Adam(\n",
    "        [\n",
    "            {\"params\": extractor, \"lr\": 1e-3},\n",
    "            {\"params\": rest, \"lr\": 1e-4},\n",
    "        ],\n",
    "    )\n",
    "    print(\"lr = \", [optim.param_groups[i]['lr'] for i in range(len(optim.param_groups))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([[0.4963, 0.7682, 0.0885],\n",
      "        [0.1320, 0.3074, 0.6341]])\n",
      "y =  tensor([[0.4901, 0.8964, 0.4556],\n",
      "        [0.6323, 0.3489, 0.4017]])\n",
      "x mag =  tensor([[0.9188],\n",
      "        [0.7169]])\n",
      "y mag =  tensor([[1.1187],\n",
      "        [0.8264]])\n",
      "xy max =  tensor([[1.1187],\n",
      "        [0.8264]])\n",
      "euclid =  tensor([[0.3889],\n",
      "        [0.5532]])\n",
      "norm euclid =  tensor([[0.3477],\n",
      "        [0.6694]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from model.vision_transformer.dual_cnn_vit.pairwise_dual_cnn_vit import PairwiseDualCNNViT\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,device, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "        print(\"output1: \", torch.norm(output1))\n",
    "        print(\"euclidean_distance: \", euclidean_distance)\n",
    "\n",
    "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      label * torch.max(torch.tensor(0.0).to(self.device), torch.pow(torch.tensor(self.margin).to(self.device) - euclidean_distance, 2)))\n",
    "        return loss_contrastive\n",
    "\n",
    "# device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "# model = PairwiseDualEfficientViT( image_size=128, num_classes=1, dim=1024,\\\n",
    "#                                 depth=6, heads=8, mlp_dim=2048,\\\n",
    "#                                 dim_head=64, dropout=0.15, emb_dropout=0.15,\\\n",
    "#                                 backbone='efficient_net', pretrained=True,\\\n",
    "#                                 normalize_ifft=False,\\\n",
    "#                                 flatten_type='patch',\\\n",
    "#                                 conv_attn=False, ratio=1, qkv_embed=False, inner_ca_dim=0, init_ca_weight=False, prj_out=False, act='none',\\\n",
    "#                                 patch_size=1, position_embed=True, pool='cls',\\\n",
    "#                                 version='ca-fadd-0.8', unfreeze_blocks=-1)\n",
    "\n",
    "# x0, y0 = torch.ones(1, 3, 128, 128), torch.ones(1, 1, 128, 128)\n",
    "# x1, y1 = torch.ones(1, 3, 128, 128), torch.ones(1, 1, 128, 128)\n",
    "\n",
    "# mode = model.to(device)\n",
    "# x0, y0, x1, y1 = x0.to(device), y0.to(device), x1.to(device), y1.to(device)\n",
    "# em0, out0, em1, out1 = model(x0, y0, x1, y1)\n",
    "# loss = ContrastiveLoss(device=device, margin=2)\n",
    "# loss(em0, em1, 0)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand((2, 3))\n",
    "y = torch.rand((2, 3))\n",
    "\n",
    "x_magnitude = torch.norm(x, dim=1, keepdim=True)\n",
    "y_magnitude = torch.norm(y, dim=1, keepdim=True)\n",
    "max_xy = torch.maximum(x_magnitude, y_magnitude)\n",
    "euclidean_distance = F.pairwise_distance(x, y, keepdim=True)\n",
    "norm_euclidean_distance = euclidean_distance / max_xy\n",
    "print(\"x = \", x)\n",
    "print(\"y = \", y)\n",
    "print(\"x mag = \", x_magnitude)\n",
    "print(\"y mag = \", y_magnitude)\n",
    "print(\"xy max = \", max_xy)\n",
    "print(\"euclid = \", euclidean_distance)\n",
    "print(\"norm euclid = \", norm_euclidean_distance)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.gen_dual_fft import *\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "image_size=128\n",
    "transform_test_fwd = transforms.Compose([transforms.Resize((image_size,image_size)),\\\n",
    "                                        transforms.ToTensor(), \\\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                        ])\n",
    "transform_fft = transforms.Compose([transforms.ToTensor()])    \n",
    "\n",
    "fftset = ImageGeneratorDualFFTFeature(path=\"/mnt/disk1/doan/phucnp/Dataset/UADFV/image/train\", image_size=image_size, transform=transform_test_fwd, transform_fft=transform_fft)\n",
    "\n",
    "fftloader = torch.utils.data.DataLoader(fftset, batch_size=16, num_workers=4, shuffle=True)\n",
    "from model.vision_transformer.dual_cnn_feedfoward_vit import DualCNNFeedForwardViT\n",
    "\n",
    "model_ = DualCNNFeedForwardViT(  image_size=128, num_classes=1, dim=1024,\\\n",
    "                            depth=6, heads=8, mlp_dim=2048,\\\n",
    "                            dim_head=64, dropout=0.15, emb_dropout=0.15,\\\n",
    "                            backbone='xception_net', pretrained=False, unfreeze_blocks=-1,\\\n",
    "                            conv_reduction_channels=False, ratio_reduction=1,\\\n",
    "                            flatten_type='patch', patch_size=2,\\\n",
    "                            input_freq_dim=88, hidden_freq_dim=256,\\\n",
    "                            position_embed=False, pool='cls',\\\n",
    "                            aggregation=\"cat-0.8\",\\\n",
    "                            init_weight=False, init_linear=\"xavier\", init_layernorm=\"norm\", init_conv=\"kaiming\")\n",
    "\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "model_ = model_.to(device)\n",
    "for imgs, ffts, labels in fftloader:\n",
    "    imgs, ffts = imgs.float().to(device), ffts.float().to(device)\n",
    "    out = model_(imgs, ffts)\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from dataloader.transform import AddGaussianNoise\n",
    "import os.path as osp\n",
    "import os\n",
    "\n",
    "\n",
    "for img_path in os.listdir(\"/mnt/disk1/doan/phucnp/Dataset/df_in_the_wildv4/image/test/0_real/\")[:10]:\n",
    "    img = Image.open(osp.join(\"/mnt/disk1/doan/phucnp/Dataset/df_in_the_wildv4/image/test/0_real/\", img_path))\n",
    "    plt.figure(figsize=(30, 50))\n",
    "    plt.subplot(10, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    # plt.show()\n",
    "    img = transform(img)\n",
    "    # print(img.shape)\n",
    "    plt.subplot(10, 2, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Pretrained backbone:  True\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Pretrained backbone:  True\n",
      "torch.Size([32, 64, 4, 4])\n",
      "torch.Size([32, 4, 1024])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit.vit import ViT, Transformer\n",
    "from model.vision_transformer.cnn_vit.efficient_vit import EfficientViT\n",
    "from pytorchcv.model_provider import get_model\n",
    "from model.backbone.efficient_net.utils import Conv2dStaticSamePadding\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\" CMA attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, activation=None, ratio=8, cross_value=True, gamma_cma=-1):\n",
    "        super().__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.activation = activation\n",
    "        self.cross_value = cross_value\n",
    "\n",
    "        self.query_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//ratio, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//ratio, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        if gamma_cma == -1:\n",
    "            self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        else:\n",
    "            self.gamma = gamma_cma\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature\n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        proj_query = self.query_conv(x).view(\n",
    "            B, -1, H*W).permute(0, 2, 1)  # B , HW, C\n",
    "        proj_key = self.key_conv(y).view(\n",
    "            B, -1, H*W)  # B X C x (*W*H)\n",
    "        energy = torch.bmm(proj_query, proj_key)  # B, HW, HW\n",
    "        attention = self.softmax(energy)  # BX (N) X (N)\n",
    "        if self.cross_value:\n",
    "            proj_value = self.value_conv(z).view(\n",
    "                B, -1, H*W)  # B , C , HW\n",
    "        else:\n",
    "            proj_value = self.value_conv(z).view(\n",
    "                B, -1, H*W)  # B , C , HW\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(B, C, H, W)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "\n",
    "        if self.activation is not None:\n",
    "            out = self.activation(out)\n",
    "        # print(\"out: \", out.shape)\n",
    "        return out  # , attention\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patchsize, d_model):\n",
    "        super().__init__()\n",
    "        self.patchsize = patchsize\n",
    "        self.query_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.value_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.key_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(d_model),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(\n",
    "            query.size(-1)\n",
    "        )\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        p_val = torch.matmul(p_attn, value)\n",
    "        return p_val, p_attn\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()   # 32, 1280, 8, 8\n",
    "        # print(\"x size:\", x.size())\n",
    "        d_k = c // len(self.patchsize)  # 320\n",
    "        output = []\n",
    "        _query = self.query_embedding(x)\n",
    "        _key = self.key_embedding(x)\n",
    "        _value = self.value_embedding(x)\n",
    "        attentions = []\n",
    "        # print(\"_query: \", _query.shape)\n",
    "        for (width, height), query, key, value in zip(\n",
    "            self.patchsize,\n",
    "            torch.chunk(_query, len(self.patchsize), dim=1),\n",
    "            torch.chunk(_key, len(self.patchsize), dim=1),\n",
    "            torch.chunk(_value, len(self.patchsize), dim=1),\n",
    "        ):\n",
    "            # print('query: ', query.shape)   # (B, )\n",
    "            out_w, out_h = w // width, h // height\n",
    "\n",
    "            # 1) embedding and reshape\n",
    "            query = query.view(b, d_k, out_h, height, out_w, width)\n",
    "            query = (\n",
    "                query.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "            key = key.view(b, d_k, out_h, height, out_w, width)\n",
    "            key = (\n",
    "                key.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "            value = value.view(b, d_k, out_h, height, out_w, width)\n",
    "            value = (\n",
    "                value.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "\n",
    "            y, _ = self.attention(query, key, value)\n",
    "\n",
    "            # 3) \"Concat\" using a view and apply a final linear.\n",
    "            y = y.view(b, out_h, out_w, d_k, height, width)\n",
    "            y = y.permute(0, 3, 1, 4, 2, 5).contiguous().view(b, d_k, h, w)\n",
    "            attentions.append(y)\n",
    "            output.append(y)\n",
    "\n",
    "        output = torch.cat(output, 1)\n",
    "        self_attention = self.output_linear(output)\n",
    "\n",
    "        return self_attention\n",
    "\n",
    "class MultiHeadedAttentionv2(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patchsize, d_model):\n",
    "        super().__init__()\n",
    "        self.patchsize = patchsize\n",
    "        self.query_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.value_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.key_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(d_model),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(\n",
    "            query.size(-1)\n",
    "        )\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        p_val = torch.matmul(p_attn, value)\n",
    "        return p_val, p_attn\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        b, c, h, w = x.size()   # 32, 1280, 8, 8\n",
    "        # print(\"x size:\", x.size())\n",
    "        d_k = c // len(self.patchsize)  # 320\n",
    "        output = []\n",
    "        _query = self.query_embedding(x)\n",
    "        _key = self.key_embedding(y)\n",
    "        _value = self.value_embedding(y)\n",
    "        attentions = []\n",
    "        # print(\"_query: \", _query.shape)\n",
    "        for (width, height), query, key, value in zip(\n",
    "            self.patchsize,\n",
    "            torch.chunk(_query, len(self.patchsize), dim=1),\n",
    "            torch.chunk(_key, len(self.patchsize), dim=1),\n",
    "            torch.chunk(_value, len(self.patchsize), dim=1),\n",
    "        ):\n",
    "            # print('query: ', query.shape)   # (B, )\n",
    "            out_w, out_h = w // width, h // height\n",
    "\n",
    "            # 1) embedding and reshape\n",
    "            query = query.view(b, d_k, out_h, height, out_w, width)\n",
    "            query = (\n",
    "                query.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "            key = key.view(b, d_k, out_h, height, out_w, width)\n",
    "            key = (\n",
    "                key.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "            value = value.view(b, d_k, out_h, height, out_w, width)\n",
    "            value = (\n",
    "                value.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "\n",
    "            out, attn = self.attention(query, key, value)\n",
    "\n",
    "            # 3) \"Concat\" using a view and apply a final linear.\n",
    "            out = out.view(b, out_h, out_w, d_k, height, width)\n",
    "            out = out.permute(0, 3, 1, 4, 2, 5).contiguous().view(b, d_k, h, w)\n",
    "            attentions.append(attn)\n",
    "            output.append(out)\n",
    "\n",
    "        output = torch.cat(output, 1)\n",
    "        self_attention = self.output_linear(output)\n",
    "        return self_attention\n",
    "\n",
    "class FeedForward2D(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channel, out_channel, kernel_size=3, padding=2, dilation=2\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class PatchTrans(nn.Module):\n",
    "    def __init__(self, in_channel, in_size, patch_resolution=\"1-2-4-8\"):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "\n",
    "        patchsize = []\n",
    "        reso = map(float, patch_resolution.split(\"-\"))\n",
    "        for r in reso:\n",
    "            patchsize.append((int(in_size//r), int(in_size//r)))\n",
    "        # print(patchsize)\n",
    "        self.transform_ = TransformerBlock(patchsize, in_channel=in_channel)\n",
    "        # print(in_channel)\n",
    "\n",
    "    def forward(self, enc_feat):\n",
    "        output = self.transform_(enc_feat)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patchsize, in_channel=256):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(patchsize, d_model=in_channel)\n",
    "        self.feed_forward = FeedForward2D(\n",
    "            in_channel=in_channel, out_channel=in_channel\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb):\n",
    "        self_attention = self.attention(rgb)\n",
    "        output = rgb + self_attention\n",
    "        output = output + self.feed_forward(output)\n",
    "        return output\n",
    "\n",
    "class TransformerBlockv2(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patchsize, in_channel=256):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttentionv2(patchsize, d_model=in_channel)\n",
    "        self.feed_forward = FeedForward2D(\n",
    "            in_channel=in_channel, out_channel=in_channel\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb, freq):\n",
    "        self_attention = self.attention(rgb, freq)\n",
    "        output = rgb + self_attention\n",
    "        output = output + self.feed_forward(output)\n",
    "        return output\n",
    "\n",
    "class PatchTransv2(nn.Module):\n",
    "    def __init__(self, in_channel, in_size, patch_resolution=\"1-2-4-8\"):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "\n",
    "        patchsize = []\n",
    "        reso = map(float, patch_resolution.split(\"-\"))\n",
    "        for r in reso:\n",
    "            patchsize.append((int(in_size//r), int(in_size//r)))\n",
    "        # print(patchsize)\n",
    "        self.transform_ = TransformerBlockv2(patchsize, in_channel=in_channel)\n",
    "        # print(in_channel)\n",
    "\n",
    "    def forward(self, rgb_fea, freq_fea):\n",
    "        output = self.transform_(rgb_fea, freq_fea)\n",
    "        return output\n",
    "\n",
    "class DualPatchCNNCMAViT(nn.Module):\n",
    "    def __init__(self, image_size=224, num_classes=1, depth_block4=2, \\\n",
    "                backbone='xception_net', pretrained=True, unfreeze_blocks=-1, \\\n",
    "                normalize_ifft='batchnorm',\\\n",
    "                act='none',\\\n",
    "                init_type=\"xavier_uniform\", \\\n",
    "                gamma_cma=-1, flatten_type='patch', patch_size=2, \\\n",
    "                dim=1024, depth_vit=2, heads=3, dim_head=64, dropout=0.15, emb_dropout=0.15, mlp_dim=2048, dropout_in_mlp=0.0, \\\n",
    "                classifier='mlp', in_vit_channels=64):  \n",
    "        super(DualPatchCNNCMAViT, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.depth_block4 = depth_block4\n",
    "\n",
    "        self.depth_vit = depth_vit\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.flatten_type = flatten_type\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.features_size = {\n",
    "            'efficient_net': (1280, 8, 8),\n",
    "            'xception_net': (2048, 8, 8),\n",
    "        }\n",
    "        self.out_ext_channels = self.features_size[backbone][0]\n",
    "        \n",
    "        # self.flatten_type = flatten_type # in ['patch', 'channel']\n",
    "        # self.version = version  # in ['ca-rgb_cat-0.5', 'ca-freq_cat-0.5']\n",
    "        # self.position_embed = position_embed\n",
    "        # self.pool = pool\n",
    "        # self.conv_attn = conv_attn\n",
    "        self.activation = self.get_activation(act)\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.rgb_extractor = self.get_feature_extractor(architecture=backbone, pretrained=pretrained, unfreeze_blocks=unfreeze_blocks, num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(architecture=backbone, pretrained=pretrained, unfreeze_blocks=unfreeze_blocks, num_classes=num_classes, in_channels=1)     \n",
    "        self.normalize_ifft = normalize_ifft\n",
    "        if self.normalize_ifft == 'batchnorm':\n",
    "            self.batchnorm_ifft = nn.BatchNorm2d(num_features=self.out_ext_channels if classifier == 'mlp' else 320)\n",
    "        if self.normalize_ifft == 'layernorm':\n",
    "            self.layernorm_ifft = nn.LayerNorm(normalized_shape=self.features_size[self.backbone])\n",
    "        ############################# PATCH CONFIG ################################\n",
    "\n",
    "        # self.CA = CrossAttention(in_dim=self.in_dim, inner_dim=inner_ca_dim, prj_out=prj_out, qkv_embed=qkv_embed, init_weight=init_ca_weight)\n",
    "        device = torch.device('cpu')\n",
    "        self.cma = CrossModalAttention(in_dim=self.out_ext_channels if classifier=='mlp' else 320, activation=self.activation, ratio=4, cross_value=True, gamma_cma=gamma_cma).to(device)\n",
    "\n",
    "        # Thêm 1 embedding vector cho classify token:\n",
    "        # self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "        # self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer_block_4 = nn.ModuleList([])\n",
    "        for _ in range(depth_block4):\n",
    "            self.transformer_block_4.append(PatchTrans(in_channel=40, in_size=16, patch_resolution='1-2-4-8').to(device))\n",
    "        self.transformer_block_10_rgb = PatchTransv2(in_channel=112, in_size=8, patch_resolution='1-2-4-8').to(device)\n",
    "        self.transformer_block_10_freq = PatchTransv2(in_channel=112, in_size=8, patch_resolution='1-2-4-8').to(device)\n",
    "\n",
    "        # Classifier:\n",
    "        self.classifier = classifier\n",
    "        if self.classifier == 'mlp':\n",
    "            self.mlp_head = nn.Sequential(\n",
    "                nn.Dropout(dropout_in_mlp),\n",
    "                nn.Linear(1280, self.mlp_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_in_mlp),\n",
    "                nn.Linear(self.mlp_dim, self.num_classes)\n",
    "            )\n",
    "        if self.classifier == 'vit':\n",
    "            self.convr = nn.Conv2d(in_channels=320, out_channels=in_vit_channels, kernel_size=1)\n",
    "            self.embedding = nn.Linear(self.patch_size*self.patch_size *in_vit_channels if flatten_type=='patch' else 16, self.dim)\n",
    "            self.dropout = nn.Dropout(self.emb_dropout)\n",
    "            self.transformer = Transformer(self.dim, self.depth_vit, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "            self.mlp_head = nn.Sequential(\n",
    "                nn.Dropout(dropout_in_mlp),\n",
    "                nn.Linear(self.dim, self.mlp_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_in_mlp),\n",
    "                nn.Linear(self.mlp_dim, self.num_classes)\n",
    "            )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.init_weights(init_type=init_type)\n",
    "\n",
    "    def get_activation(self, act):\n",
    "        if act == 'relu':\n",
    "            activation = nn.ReLU(inplace=True)\n",
    "        elif act == 'leakyrelu':\n",
    "            activation = nn.LeakyReLU(0.01, inplace=True)\n",
    "        elif act == 'tanh':\n",
    "            activation = nn.Tanh()\n",
    "        elif act == 'sigmoid':\n",
    "            activation = nn.Sigmoid()\n",
    "        elif act == 'selu':\n",
    "            activation = nn.SELU()\n",
    "        else:\n",
    "            activation = None\n",
    "        return activation\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", unfreeze_blocks=-1, pretrained=False, num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels, pretrained=bool(pretrained))\n",
    "            # extractor._blocks[11]._depthwise_conv = Conv2dStaticSamePadding(in_channels=672, out_channels=672, kernel_size=(5, 5), stride=(1, 1), groups=672, image_size=224)\n",
    "            # extractor._conv_head = nn.Identity()\n",
    "            if unfreeze_blocks != -1:\n",
    "                # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "                for i in range(0, len(extractor._blocks)):\n",
    "                    for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "                        if i >= len(extractor._blocks) - unfreeze_blocks:\n",
    "                            param.requires_grad = True\n",
    "                        else:\n",
    "                            param.requires_grad = False\n",
    "            # print(extractor)\n",
    "        \n",
    "        if architecture == 'xception_net':\n",
    "            xception = get_model(\"xception\", pretrained=bool(pretrained))\n",
    "            extractor = nn.Sequential(*list(xception.children())[:-1])\n",
    "            extractor[0].final_block.pool = nn.Identity()\n",
    "            if in_channels != 3:\n",
    "                extractor[0].init_block.conv1.conv = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "\n",
    "            if unfreeze_blocks != -1:\n",
    "                blocks = len(extractor[0].children())\n",
    "                print(\"Number of blocks in xception: \", len(blocks))\n",
    "                for i, block in enumerate(extractor[0].children()):\n",
    "                    if i >= blocks - unfreeze_blocks:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    else:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = False\n",
    "        print(\"Pretrained backbone: \", bool(pretrained))\n",
    "        # if not pretrained:\n",
    "        #     self.init_conv_weight(extractor)\n",
    "        return extractor\n",
    "\n",
    "    def init_weights(self, init_type='normal', gain=0.02):\n",
    "        '''\n",
    "        initialize network's weights\n",
    "        init_type: normal | xavier | kaiming | orthogonal\n",
    "        https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9451e70673400885567d08a9e97ade2524c700d0/models/networks.py#L39\n",
    "        '''\n",
    "\n",
    "        def init_func(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if classname.find('InstanceNorm2d') != -1:\n",
    "                if hasattr(m, 'weight') and m.weight is not None:\n",
    "                    nn.init.constant_(m.weight.data, 1.0)\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "            elif hasattr(m, 'weight') and (\n",
    "                classname.find('Conv') != -1 or classname.find('Linear') != -1\n",
    "            ):\n",
    "                if init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight.data, 0.0, gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "                elif init_type == 'xavier_uniform':\n",
    "                    nn.init.xavier_uniform_(m.weight.data, gain=1.0)\n",
    "                elif init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    nn.init.orthogonal_(m.weight.data, gain=gain)\n",
    "                elif init_type == 'none':  # uses pytorch's default init method\n",
    "                    m.reset_parameters()\n",
    "                else:\n",
    "                    raise NotImplementedError(\n",
    "                        'initialization method [%s] is not implemented'\n",
    "                        % init_type\n",
    "                    )\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        self.apply(init_func)\n",
    "\n",
    "        for m in self.children():\n",
    "            if hasattr(m, 'init_weights'):\n",
    "                m.init_weights(init_type, gain)\n",
    "\n",
    "    def ifft(self, freq_feature, norm_type='none'):\n",
    "        ifreq_feature = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_feature))) + 1e-10)  # Hơi ảo???\n",
    "        if norm_type == 'none':\n",
    "            pass\n",
    "        elif norm_type == 'batchnorm':\n",
    "            ifreq_feature = self.batchnorm_ifft(ifreq_feature)\n",
    "        elif norm_type == 'layernorm':\n",
    "            ifreq_feature = self.layernorm_ifft(ifreq_feature)\n",
    "        elif norm_type == 'normal':\n",
    "            ifreq_feature = F.normalize(ifreq_feature)\n",
    "        return ifreq_feature\n",
    "\n",
    "    def flatten_to_vectors(self, feature):\n",
    "        vectors = None\n",
    "        if self.flatten_type == 'patch':\n",
    "            vectors = rearrange(feature, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "        elif self.flatten_type == 'channel':\n",
    "            vectors = rearrange(feature, 'b c h w -> b c (h w)')\n",
    "        else:\n",
    "            pass\n",
    "        return vectors\n",
    "\n",
    "    def extract_feature(self, rgb_imgs, freq_imgs):\n",
    "        if self.backbone == 'efficient_net':\n",
    "            #\n",
    "            rgb_features = self.rgb_extractor.extract_features_block_4(rgb_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "            for attn in self.transformer_block_4:\n",
    "                rgb_features = attn(rgb_features)\n",
    "            freq_features = self.freq_extractor.extract_features_block_4(freq_imgs)              # shape (batchsize, 1280, 4, 4)\n",
    "            #\n",
    "            rgb_features = self.rgb_extractor.extract_features_block_11(rgb_features)\n",
    "            freq_features = self.freq_extractor.extract_features_block_11(freq_features)\n",
    "            rgb_features_1 = self.transformer_block_10_rgb(rgb_features, freq_features)\n",
    "            freq_features_1 = self.transformer_block_10_freq(freq_features, rgb_features)\n",
    "            rgb_features = self.rgb_extractor.extract_features_last_block(rgb_features_1, classifier=self.classifier)\n",
    "            freq_features = self.freq_extractor.extract_features_last_block(freq_features_1, classifier=self.classifier)\n",
    "        else:\n",
    "            rgb_features = self.rgb_extractor(rgb_imgs)\n",
    "            freq_features = self.freq_extractor(freq_imgs)\n",
    "        return rgb_features, freq_features\n",
    "\n",
    "    def forward(self, rgb_imgs, freq_imgs):\n",
    "        rgb_features, freq_features = self.extract_feature(rgb_imgs, freq_imgs)\n",
    "        ifreq_features = self.ifft(freq_features, norm_type=self.normalize_ifft)\n",
    "        # print(\"Features shape: \", rgb_features.shape, freq_features.shape, ifreq_features.shape)\n",
    "        out = self.cma(rgb_features, ifreq_features, freq_features)\n",
    "\n",
    "        if self.classifier == 'mlp':\n",
    "            x = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "            x = x.squeeze().squeeze()\n",
    "            x = self.mlp_head(x)\n",
    "\n",
    "        if self.classifier == 'vit':\n",
    "            x = self.convr(out)\n",
    "            print(x.shape)\n",
    "            x = self.flatten_to_vectors(x)\n",
    "            x = self.embedding(x)\n",
    "            print(x.shape)\n",
    "            x = self.dropout(x)\n",
    "            x = self.transformer(x)\n",
    "            x = x.mean(dim = 1)\n",
    "            x = self.mlp_head(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "from torchsummary import summary\n",
    "if __name__ == '__main__':\n",
    "    x = torch.ones(32, 3, 128, 128)\n",
    "    y = torch.ones(32, 1, 128, 128)\n",
    "    model_ = DualPatchCNNCMAViT(image_size=128, num_classes=1, depth_block4=2, \\\n",
    "                backbone='efficient_net', pretrained=True, unfreeze_blocks=-1, \\\n",
    "                normalize_ifft='batchnorm',\\\n",
    "                act='selu',\\\n",
    "                init_type=\"xavier_uniform\", \\\n",
    "                gamma_cma=-1, flatten_type='patch', patch_size=2, \\\n",
    "                    \n",
    "                dim=1024, depth_vit=2, heads=3, dim_head=64, dropout=0.15, emb_dropout=0.15, mlp_dim=2048, dropout_in_mlp=0.0, \\\n",
    "                classifier='vit', in_vit_channels=64)\n",
    "    out = model_(x, y)\n",
    "    print(out.shape)\n",
    "    # summary(model_, [(3, 128, 128), (1, 128, 128)], batch_size=32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Pretrained backbone:  True\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Pretrained backbone:  True\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         ZeroPad2d-1          [24, 3, 129, 129]               0\n",
      "Conv2dStaticSamePadding-2           [24, 32, 64, 64]             864\n",
      "       BatchNorm2d-3           [24, 32, 64, 64]              64\n",
      "MemoryEfficientSwish-4           [24, 32, 64, 64]               0\n",
      "         ZeroPad2d-5           [24, 32, 66, 66]               0\n",
      "Conv2dStaticSamePadding-6           [24, 32, 64, 64]             288\n",
      "       BatchNorm2d-7           [24, 32, 64, 64]              64\n",
      "MemoryEfficientSwish-8           [24, 32, 64, 64]               0\n",
      "          Identity-9             [24, 32, 1, 1]               0\n",
      "Conv2dStaticSamePadding-10              [24, 8, 1, 1]             264\n",
      "MemoryEfficientSwish-11              [24, 8, 1, 1]               0\n",
      "         Identity-12              [24, 8, 1, 1]               0\n",
      "Conv2dStaticSamePadding-13             [24, 32, 1, 1]             288\n",
      "         Identity-14           [24, 32, 64, 64]               0\n",
      "Conv2dStaticSamePadding-15           [24, 16, 64, 64]             512\n",
      "      BatchNorm2d-16           [24, 16, 64, 64]              32\n",
      "      MBConvBlock-17           [24, 16, 64, 64]               0\n",
      "         Identity-18           [24, 16, 64, 64]               0\n",
      "Conv2dStaticSamePadding-19           [24, 96, 64, 64]           1,536\n",
      "      BatchNorm2d-20           [24, 96, 64, 64]             192\n",
      "MemoryEfficientSwish-21           [24, 96, 64, 64]               0\n",
      "        ZeroPad2d-22           [24, 96, 65, 65]               0\n",
      "Conv2dStaticSamePadding-23           [24, 96, 32, 32]             864\n",
      "      BatchNorm2d-24           [24, 96, 32, 32]             192\n",
      "MemoryEfficientSwish-25           [24, 96, 32, 32]               0\n",
      "         Identity-26             [24, 96, 1, 1]               0\n",
      "Conv2dStaticSamePadding-27              [24, 4, 1, 1]             388\n",
      "MemoryEfficientSwish-28              [24, 4, 1, 1]               0\n",
      "         Identity-29              [24, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-30             [24, 96, 1, 1]             480\n",
      "         Identity-31           [24, 96, 32, 32]               0\n",
      "Conv2dStaticSamePadding-32           [24, 24, 32, 32]           2,304\n",
      "      BatchNorm2d-33           [24, 24, 32, 32]              48\n",
      "      MBConvBlock-34           [24, 24, 32, 32]               0\n",
      "         Identity-35           [24, 24, 32, 32]               0\n",
      "Conv2dStaticSamePadding-36          [24, 144, 32, 32]           3,456\n",
      "      BatchNorm2d-37          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-38          [24, 144, 32, 32]               0\n",
      "        ZeroPad2d-39          [24, 144, 34, 34]               0\n",
      "Conv2dStaticSamePadding-40          [24, 144, 32, 32]           1,296\n",
      "      BatchNorm2d-41          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-42          [24, 144, 32, 32]               0\n",
      "         Identity-43            [24, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-44              [24, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-45              [24, 6, 1, 1]               0\n",
      "         Identity-46              [24, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-47            [24, 144, 1, 1]           1,008\n",
      "         Identity-48          [24, 144, 32, 32]               0\n",
      "Conv2dStaticSamePadding-49           [24, 24, 32, 32]           3,456\n",
      "      BatchNorm2d-50           [24, 24, 32, 32]              48\n",
      "      MBConvBlock-51           [24, 24, 32, 32]               0\n",
      "         Identity-52           [24, 24, 32, 32]               0\n",
      "Conv2dStaticSamePadding-53          [24, 144, 32, 32]           3,456\n",
      "      BatchNorm2d-54          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-55          [24, 144, 32, 32]               0\n",
      "        ZeroPad2d-56          [24, 144, 35, 35]               0\n",
      "Conv2dStaticSamePadding-57          [24, 144, 16, 16]           3,600\n",
      "      BatchNorm2d-58          [24, 144, 16, 16]             288\n",
      "MemoryEfficientSwish-59          [24, 144, 16, 16]               0\n",
      "         Identity-60            [24, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-61              [24, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-62              [24, 6, 1, 1]               0\n",
      "         Identity-63              [24, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-64            [24, 144, 1, 1]           1,008\n",
      "         Identity-65          [24, 144, 16, 16]               0\n",
      "Conv2dStaticSamePadding-66           [24, 40, 16, 16]           5,760\n",
      "      BatchNorm2d-67           [24, 40, 16, 16]              80\n",
      "      MBConvBlock-68           [24, 40, 16, 16]               0\n",
      "         Identity-69           [24, 40, 16, 16]               0\n",
      "Conv2dStaticSamePadding-70          [24, 240, 16, 16]           9,600\n",
      "      BatchNorm2d-71          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-72          [24, 240, 16, 16]               0\n",
      "        ZeroPad2d-73          [24, 240, 20, 20]               0\n",
      "Conv2dStaticSamePadding-74          [24, 240, 16, 16]           6,000\n",
      "      BatchNorm2d-75          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-76          [24, 240, 16, 16]               0\n",
      "         Identity-77            [24, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-78             [24, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-79             [24, 10, 1, 1]               0\n",
      "         Identity-80             [24, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-81            [24, 240, 1, 1]           2,640\n",
      "         Identity-82          [24, 240, 16, 16]               0\n",
      "Conv2dStaticSamePadding-83           [24, 40, 16, 16]           9,600\n",
      "      BatchNorm2d-84           [24, 40, 16, 16]              80\n",
      "      MBConvBlock-85           [24, 40, 16, 16]               0\n",
      "           Conv2d-86           [24, 40, 16, 16]           1,640\n",
      "           Conv2d-87           [24, 40, 16, 16]           1,640\n",
      "           Conv2d-88           [24, 40, 16, 16]           1,640\n",
      "           Conv2d-89           [24, 40, 16, 16]          14,440\n",
      "      BatchNorm2d-90           [24, 40, 16, 16]              80\n",
      "        LeakyReLU-91           [24, 40, 16, 16]               0\n",
      "MultiHeadedAttention-92           [24, 40, 16, 16]               0\n",
      "           Conv2d-93           [24, 40, 16, 16]          14,440\n",
      "      BatchNorm2d-94           [24, 40, 16, 16]              80\n",
      "        LeakyReLU-95           [24, 40, 16, 16]               0\n",
      "           Conv2d-96           [24, 40, 16, 16]          14,440\n",
      "      BatchNorm2d-97           [24, 40, 16, 16]              80\n",
      "        LeakyReLU-98           [24, 40, 16, 16]               0\n",
      "    FeedForward2D-99           [24, 40, 16, 16]               0\n",
      "TransformerBlock-100           [24, 40, 16, 16]               0\n",
      "      PatchTrans-101           [24, 40, 16, 16]               0\n",
      "          Conv2d-102           [24, 40, 16, 16]           1,640\n",
      "          Conv2d-103           [24, 40, 16, 16]           1,640\n",
      "          Conv2d-104           [24, 40, 16, 16]           1,640\n",
      "          Conv2d-105           [24, 40, 16, 16]          14,440\n",
      "     BatchNorm2d-106           [24, 40, 16, 16]              80\n",
      "       LeakyReLU-107           [24, 40, 16, 16]               0\n",
      "MultiHeadedAttention-108           [24, 40, 16, 16]               0\n",
      "          Conv2d-109           [24, 40, 16, 16]          14,440\n",
      "     BatchNorm2d-110           [24, 40, 16, 16]              80\n",
      "       LeakyReLU-111           [24, 40, 16, 16]               0\n",
      "          Conv2d-112           [24, 40, 16, 16]          14,440\n",
      "     BatchNorm2d-113           [24, 40, 16, 16]              80\n",
      "       LeakyReLU-114           [24, 40, 16, 16]               0\n",
      "   FeedForward2D-115           [24, 40, 16, 16]               0\n",
      "TransformerBlock-116           [24, 40, 16, 16]               0\n",
      "      PatchTrans-117           [24, 40, 16, 16]               0\n",
      "       ZeroPad2d-118          [24, 1, 129, 129]               0\n",
      "Conv2dStaticSamePadding-119           [24, 32, 64, 64]             288\n",
      "     BatchNorm2d-120           [24, 32, 64, 64]              64\n",
      "MemoryEfficientSwish-121           [24, 32, 64, 64]               0\n",
      "       ZeroPad2d-122           [24, 32, 66, 66]               0\n",
      "Conv2dStaticSamePadding-123           [24, 32, 64, 64]             288\n",
      "     BatchNorm2d-124           [24, 32, 64, 64]              64\n",
      "MemoryEfficientSwish-125           [24, 32, 64, 64]               0\n",
      "        Identity-126             [24, 32, 1, 1]               0\n",
      "Conv2dStaticSamePadding-127              [24, 8, 1, 1]             264\n",
      "MemoryEfficientSwish-128              [24, 8, 1, 1]               0\n",
      "        Identity-129              [24, 8, 1, 1]               0\n",
      "Conv2dStaticSamePadding-130             [24, 32, 1, 1]             288\n",
      "        Identity-131           [24, 32, 64, 64]               0\n",
      "Conv2dStaticSamePadding-132           [24, 16, 64, 64]             512\n",
      "     BatchNorm2d-133           [24, 16, 64, 64]              32\n",
      "     MBConvBlock-134           [24, 16, 64, 64]               0\n",
      "        Identity-135           [24, 16, 64, 64]               0\n",
      "Conv2dStaticSamePadding-136           [24, 96, 64, 64]           1,536\n",
      "     BatchNorm2d-137           [24, 96, 64, 64]             192\n",
      "MemoryEfficientSwish-138           [24, 96, 64, 64]               0\n",
      "       ZeroPad2d-139           [24, 96, 65, 65]               0\n",
      "Conv2dStaticSamePadding-140           [24, 96, 32, 32]             864\n",
      "     BatchNorm2d-141           [24, 96, 32, 32]             192\n",
      "MemoryEfficientSwish-142           [24, 96, 32, 32]               0\n",
      "        Identity-143             [24, 96, 1, 1]               0\n",
      "Conv2dStaticSamePadding-144              [24, 4, 1, 1]             388\n",
      "MemoryEfficientSwish-145              [24, 4, 1, 1]               0\n",
      "        Identity-146              [24, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-147             [24, 96, 1, 1]             480\n",
      "        Identity-148           [24, 96, 32, 32]               0\n",
      "Conv2dStaticSamePadding-149           [24, 24, 32, 32]           2,304\n",
      "     BatchNorm2d-150           [24, 24, 32, 32]              48\n",
      "     MBConvBlock-151           [24, 24, 32, 32]               0\n",
      "        Identity-152           [24, 24, 32, 32]               0\n",
      "Conv2dStaticSamePadding-153          [24, 144, 32, 32]           3,456\n",
      "     BatchNorm2d-154          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-155          [24, 144, 32, 32]               0\n",
      "       ZeroPad2d-156          [24, 144, 34, 34]               0\n",
      "Conv2dStaticSamePadding-157          [24, 144, 32, 32]           1,296\n",
      "     BatchNorm2d-158          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-159          [24, 144, 32, 32]               0\n",
      "        Identity-160            [24, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-161              [24, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-162              [24, 6, 1, 1]               0\n",
      "        Identity-163              [24, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-164            [24, 144, 1, 1]           1,008\n",
      "        Identity-165          [24, 144, 32, 32]               0\n",
      "Conv2dStaticSamePadding-166           [24, 24, 32, 32]           3,456\n",
      "     BatchNorm2d-167           [24, 24, 32, 32]              48\n",
      "     MBConvBlock-168           [24, 24, 32, 32]               0\n",
      "        Identity-169           [24, 24, 32, 32]               0\n",
      "Conv2dStaticSamePadding-170          [24, 144, 32, 32]           3,456\n",
      "     BatchNorm2d-171          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-172          [24, 144, 32, 32]               0\n",
      "       ZeroPad2d-173          [24, 144, 35, 35]               0\n",
      "Conv2dStaticSamePadding-174          [24, 144, 16, 16]           3,600\n",
      "     BatchNorm2d-175          [24, 144, 16, 16]             288\n",
      "MemoryEfficientSwish-176          [24, 144, 16, 16]               0\n",
      "        Identity-177            [24, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-178              [24, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-179              [24, 6, 1, 1]               0\n",
      "        Identity-180              [24, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-181            [24, 144, 1, 1]           1,008\n",
      "        Identity-182          [24, 144, 16, 16]               0\n",
      "Conv2dStaticSamePadding-183           [24, 40, 16, 16]           5,760\n",
      "     BatchNorm2d-184           [24, 40, 16, 16]              80\n",
      "     MBConvBlock-185           [24, 40, 16, 16]               0\n",
      "        Identity-186           [24, 40, 16, 16]               0\n",
      "Conv2dStaticSamePadding-187          [24, 240, 16, 16]           9,600\n",
      "     BatchNorm2d-188          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-189          [24, 240, 16, 16]               0\n",
      "       ZeroPad2d-190          [24, 240, 20, 20]               0\n",
      "Conv2dStaticSamePadding-191          [24, 240, 16, 16]           6,000\n",
      "     BatchNorm2d-192          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-193          [24, 240, 16, 16]               0\n",
      "        Identity-194            [24, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-195             [24, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-196             [24, 10, 1, 1]               0\n",
      "        Identity-197             [24, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-198            [24, 240, 1, 1]           2,640\n",
      "        Identity-199          [24, 240, 16, 16]               0\n",
      "Conv2dStaticSamePadding-200           [24, 40, 16, 16]           9,600\n",
      "     BatchNorm2d-201           [24, 40, 16, 16]              80\n",
      "     MBConvBlock-202           [24, 40, 16, 16]               0\n",
      "        Identity-203           [24, 40, 16, 16]               0\n",
      "Conv2dStaticSamePadding-204          [24, 240, 16, 16]           9,600\n",
      "     BatchNorm2d-205          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-206          [24, 240, 16, 16]               0\n",
      "       ZeroPad2d-207          [24, 240, 17, 17]               0\n",
      "Conv2dStaticSamePadding-208            [24, 240, 8, 8]           2,160\n",
      "     BatchNorm2d-209            [24, 240, 8, 8]             480\n",
      "MemoryEfficientSwish-210            [24, 240, 8, 8]               0\n",
      "        Identity-211            [24, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-212             [24, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-213             [24, 10, 1, 1]               0\n",
      "        Identity-214             [24, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-215            [24, 240, 1, 1]           2,640\n",
      "        Identity-216            [24, 240, 8, 8]               0\n",
      "Conv2dStaticSamePadding-217             [24, 80, 8, 8]          19,200\n",
      "     BatchNorm2d-218             [24, 80, 8, 8]             160\n",
      "     MBConvBlock-219             [24, 80, 8, 8]               0\n",
      "        Identity-220             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-221            [24, 480, 8, 8]          38,400\n",
      "     BatchNorm2d-222            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-223            [24, 480, 8, 8]               0\n",
      "       ZeroPad2d-224          [24, 480, 10, 10]               0\n",
      "Conv2dStaticSamePadding-225            [24, 480, 8, 8]           4,320\n",
      "     BatchNorm2d-226            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-227            [24, 480, 8, 8]               0\n",
      "        Identity-228            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-229             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-230             [24, 20, 1, 1]               0\n",
      "        Identity-231             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-232            [24, 480, 1, 1]          10,080\n",
      "        Identity-233            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-234             [24, 80, 8, 8]          38,400\n",
      "     BatchNorm2d-235             [24, 80, 8, 8]             160\n",
      "     MBConvBlock-236             [24, 80, 8, 8]               0\n",
      "        Identity-237             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-238            [24, 480, 8, 8]          38,400\n",
      "     BatchNorm2d-239            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-240            [24, 480, 8, 8]               0\n",
      "       ZeroPad2d-241          [24, 480, 10, 10]               0\n",
      "Conv2dStaticSamePadding-242            [24, 480, 8, 8]           4,320\n",
      "     BatchNorm2d-243            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-244            [24, 480, 8, 8]               0\n",
      "        Identity-245            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-246             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-247             [24, 20, 1, 1]               0\n",
      "        Identity-248             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-249            [24, 480, 1, 1]          10,080\n",
      "        Identity-250            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-251             [24, 80, 8, 8]          38,400\n",
      "     BatchNorm2d-252             [24, 80, 8, 8]             160\n",
      "     MBConvBlock-253             [24, 80, 8, 8]               0\n",
      "        Identity-254             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-255            [24, 480, 8, 8]          38,400\n",
      "     BatchNorm2d-256            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-257            [24, 480, 8, 8]               0\n",
      "       ZeroPad2d-258          [24, 480, 12, 12]               0\n",
      "Conv2dStaticSamePadding-259            [24, 480, 8, 8]          12,000\n",
      "     BatchNorm2d-260            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-261            [24, 480, 8, 8]               0\n",
      "        Identity-262            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-263             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-264             [24, 20, 1, 1]               0\n",
      "        Identity-265             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-266            [24, 480, 1, 1]          10,080\n",
      "        Identity-267            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-268            [24, 112, 8, 8]          53,760\n",
      "     BatchNorm2d-269            [24, 112, 8, 8]             224\n",
      "     MBConvBlock-270            [24, 112, 8, 8]               0\n",
      "        Identity-271            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-272            [24, 672, 8, 8]          75,264\n",
      "     BatchNorm2d-273            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-274            [24, 672, 8, 8]               0\n",
      "       ZeroPad2d-275          [24, 672, 12, 12]               0\n",
      "Conv2dStaticSamePadding-276            [24, 672, 8, 8]          16,800\n",
      "     BatchNorm2d-277            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-278            [24, 672, 8, 8]               0\n",
      "        Identity-279            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-280             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-281             [24, 28, 1, 1]               0\n",
      "        Identity-282             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-283            [24, 672, 1, 1]          19,488\n",
      "        Identity-284            [24, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-285            [24, 112, 8, 8]          75,264\n",
      "     BatchNorm2d-286            [24, 112, 8, 8]             224\n",
      "     MBConvBlock-287            [24, 112, 8, 8]               0\n",
      "        Identity-288            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-289            [24, 672, 8, 8]          75,264\n",
      "     BatchNorm2d-290            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-291            [24, 672, 8, 8]               0\n",
      "       ZeroPad2d-292          [24, 672, 12, 12]               0\n",
      "Conv2dStaticSamePadding-293            [24, 672, 8, 8]          16,800\n",
      "     BatchNorm2d-294            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-295            [24, 672, 8, 8]               0\n",
      "        Identity-296            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-297             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-298             [24, 28, 1, 1]               0\n",
      "        Identity-299             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-300            [24, 672, 1, 1]          19,488\n",
      "        Identity-301            [24, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-302            [24, 112, 8, 8]          75,264\n",
      "     BatchNorm2d-303            [24, 112, 8, 8]             224\n",
      "     MBConvBlock-304            [24, 112, 8, 8]               0\n",
      "        Identity-305           [24, 40, 16, 16]               0\n",
      "Conv2dStaticSamePadding-306          [24, 240, 16, 16]           9,600\n",
      "     BatchNorm2d-307          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-308          [24, 240, 16, 16]               0\n",
      "       ZeroPad2d-309          [24, 240, 17, 17]               0\n",
      "Conv2dStaticSamePadding-310            [24, 240, 8, 8]           2,160\n",
      "     BatchNorm2d-311            [24, 240, 8, 8]             480\n",
      "MemoryEfficientSwish-312            [24, 240, 8, 8]               0\n",
      "        Identity-313            [24, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-314             [24, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-315             [24, 10, 1, 1]               0\n",
      "        Identity-316             [24, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-317            [24, 240, 1, 1]           2,640\n",
      "        Identity-318            [24, 240, 8, 8]               0\n",
      "Conv2dStaticSamePadding-319             [24, 80, 8, 8]          19,200\n",
      "     BatchNorm2d-320             [24, 80, 8, 8]             160\n",
      "     MBConvBlock-321             [24, 80, 8, 8]               0\n",
      "        Identity-322             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-323            [24, 480, 8, 8]          38,400\n",
      "     BatchNorm2d-324            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-325            [24, 480, 8, 8]               0\n",
      "       ZeroPad2d-326          [24, 480, 10, 10]               0\n",
      "Conv2dStaticSamePadding-327            [24, 480, 8, 8]           4,320\n",
      "     BatchNorm2d-328            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-329            [24, 480, 8, 8]               0\n",
      "        Identity-330            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-331             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-332             [24, 20, 1, 1]               0\n",
      "        Identity-333             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-334            [24, 480, 1, 1]          10,080\n",
      "        Identity-335            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-336             [24, 80, 8, 8]          38,400\n",
      "     BatchNorm2d-337             [24, 80, 8, 8]             160\n",
      "     MBConvBlock-338             [24, 80, 8, 8]               0\n",
      "        Identity-339             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-340            [24, 480, 8, 8]          38,400\n",
      "     BatchNorm2d-341            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-342            [24, 480, 8, 8]               0\n",
      "       ZeroPad2d-343          [24, 480, 10, 10]               0\n",
      "Conv2dStaticSamePadding-344            [24, 480, 8, 8]           4,320\n",
      "     BatchNorm2d-345            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-346            [24, 480, 8, 8]               0\n",
      "        Identity-347            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-348             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-349             [24, 20, 1, 1]               0\n",
      "        Identity-350             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-351            [24, 480, 1, 1]          10,080\n",
      "        Identity-352            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-353             [24, 80, 8, 8]          38,400\n",
      "     BatchNorm2d-354             [24, 80, 8, 8]             160\n",
      "     MBConvBlock-355             [24, 80, 8, 8]               0\n",
      "        Identity-356             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-357            [24, 480, 8, 8]          38,400\n",
      "     BatchNorm2d-358            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-359            [24, 480, 8, 8]               0\n",
      "       ZeroPad2d-360          [24, 480, 12, 12]               0\n",
      "Conv2dStaticSamePadding-361            [24, 480, 8, 8]          12,000\n",
      "     BatchNorm2d-362            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-363            [24, 480, 8, 8]               0\n",
      "        Identity-364            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-365             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-366             [24, 20, 1, 1]               0\n",
      "        Identity-367             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-368            [24, 480, 1, 1]          10,080\n",
      "        Identity-369            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-370            [24, 112, 8, 8]          53,760\n",
      "     BatchNorm2d-371            [24, 112, 8, 8]             224\n",
      "     MBConvBlock-372            [24, 112, 8, 8]               0\n",
      "        Identity-373            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-374            [24, 672, 8, 8]          75,264\n",
      "     BatchNorm2d-375            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-376            [24, 672, 8, 8]               0\n",
      "       ZeroPad2d-377          [24, 672, 12, 12]               0\n",
      "Conv2dStaticSamePadding-378            [24, 672, 8, 8]          16,800\n",
      "     BatchNorm2d-379            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-380            [24, 672, 8, 8]               0\n",
      "        Identity-381            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-382             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-383             [24, 28, 1, 1]               0\n",
      "        Identity-384             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-385            [24, 672, 1, 1]          19,488\n",
      "        Identity-386            [24, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-387            [24, 112, 8, 8]          75,264\n",
      "     BatchNorm2d-388            [24, 112, 8, 8]             224\n",
      "     MBConvBlock-389            [24, 112, 8, 8]               0\n",
      "        Identity-390            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-391            [24, 672, 8, 8]          75,264\n",
      "     BatchNorm2d-392            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-393            [24, 672, 8, 8]               0\n",
      "       ZeroPad2d-394          [24, 672, 12, 12]               0\n",
      "Conv2dStaticSamePadding-395            [24, 672, 8, 8]          16,800\n",
      "     BatchNorm2d-396            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-397            [24, 672, 8, 8]               0\n",
      "        Identity-398            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-399             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-400             [24, 28, 1, 1]               0\n",
      "        Identity-401             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-402            [24, 672, 1, 1]          19,488\n",
      "        Identity-403            [24, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-404            [24, 112, 8, 8]          75,264\n",
      "     BatchNorm2d-405            [24, 112, 8, 8]             224\n",
      "     MBConvBlock-406            [24, 112, 8, 8]               0\n",
      "          Conv2d-407            [24, 112, 8, 8]          12,656\n",
      "          Conv2d-408            [24, 112, 8, 8]          12,656\n",
      "          Conv2d-409            [24, 112, 8, 8]          12,656\n",
      "          Conv2d-410            [24, 112, 8, 8]         113,008\n",
      "     BatchNorm2d-411            [24, 112, 8, 8]             224\n",
      "       LeakyReLU-412            [24, 112, 8, 8]               0\n",
      "MultiHeadedAttentionv2-413            [24, 112, 8, 8]               0\n",
      "          Conv2d-414            [24, 112, 8, 8]         113,008\n",
      "     BatchNorm2d-415            [24, 112, 8, 8]             224\n",
      "       LeakyReLU-416            [24, 112, 8, 8]               0\n",
      "          Conv2d-417            [24, 112, 8, 8]         113,008\n",
      "     BatchNorm2d-418            [24, 112, 8, 8]             224\n",
      "       LeakyReLU-419            [24, 112, 8, 8]               0\n",
      "   FeedForward2D-420            [24, 112, 8, 8]               0\n",
      "TransformerBlockv2-421            [24, 112, 8, 8]               0\n",
      "    PatchTransv2-422            [24, 112, 8, 8]               0\n",
      "          Conv2d-423            [24, 112, 8, 8]          12,656\n",
      "          Conv2d-424            [24, 112, 8, 8]          12,656\n",
      "          Conv2d-425            [24, 112, 8, 8]          12,656\n",
      "          Conv2d-426            [24, 112, 8, 8]         113,008\n",
      "     BatchNorm2d-427            [24, 112, 8, 8]             224\n",
      "       LeakyReLU-428            [24, 112, 8, 8]               0\n",
      "MultiHeadedAttentionv2-429            [24, 112, 8, 8]               0\n",
      "          Conv2d-430            [24, 112, 8, 8]         113,008\n",
      "     BatchNorm2d-431            [24, 112, 8, 8]             224\n",
      "       LeakyReLU-432            [24, 112, 8, 8]               0\n",
      "          Conv2d-433            [24, 112, 8, 8]         113,008\n",
      "     BatchNorm2d-434            [24, 112, 8, 8]             224\n",
      "       LeakyReLU-435            [24, 112, 8, 8]               0\n",
      "   FeedForward2D-436            [24, 112, 8, 8]               0\n",
      "TransformerBlockv2-437            [24, 112, 8, 8]               0\n",
      "    PatchTransv2-438            [24, 112, 8, 8]               0\n",
      "        Identity-439            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-440            [24, 672, 8, 8]          75,264\n",
      "     BatchNorm2d-441            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-442            [24, 672, 8, 8]               0\n",
      "       ZeroPad2d-443          [24, 672, 11, 11]               0\n",
      "Conv2dStaticSamePadding-444            [24, 672, 4, 4]          16,800\n",
      "     BatchNorm2d-445            [24, 672, 4, 4]           1,344\n",
      "MemoryEfficientSwish-446            [24, 672, 4, 4]               0\n",
      "        Identity-447            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-448             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-449             [24, 28, 1, 1]               0\n",
      "        Identity-450             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-451            [24, 672, 1, 1]          19,488\n",
      "        Identity-452            [24, 672, 4, 4]               0\n",
      "Conv2dStaticSamePadding-453            [24, 192, 4, 4]         129,024\n",
      "     BatchNorm2d-454            [24, 192, 4, 4]             384\n",
      "     MBConvBlock-455            [24, 192, 4, 4]               0\n",
      "        Identity-456            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-457           [24, 1152, 4, 4]         221,184\n",
      "     BatchNorm2d-458           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-459           [24, 1152, 4, 4]               0\n",
      "       ZeroPad2d-460           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-461           [24, 1152, 4, 4]          28,800\n",
      "     BatchNorm2d-462           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-463           [24, 1152, 4, 4]               0\n",
      "        Identity-464           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-465             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-466             [24, 48, 1, 1]               0\n",
      "        Identity-467             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-468           [24, 1152, 1, 1]          56,448\n",
      "        Identity-469           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-470            [24, 192, 4, 4]         221,184\n",
      "     BatchNorm2d-471            [24, 192, 4, 4]             384\n",
      "     MBConvBlock-472            [24, 192, 4, 4]               0\n",
      "        Identity-473            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-474           [24, 1152, 4, 4]         221,184\n",
      "     BatchNorm2d-475           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-476           [24, 1152, 4, 4]               0\n",
      "       ZeroPad2d-477           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-478           [24, 1152, 4, 4]          28,800\n",
      "     BatchNorm2d-479           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-480           [24, 1152, 4, 4]               0\n",
      "        Identity-481           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-482             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-483             [24, 48, 1, 1]               0\n",
      "        Identity-484             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-485           [24, 1152, 1, 1]          56,448\n",
      "        Identity-486           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-487            [24, 192, 4, 4]         221,184\n",
      "     BatchNorm2d-488            [24, 192, 4, 4]             384\n",
      "     MBConvBlock-489            [24, 192, 4, 4]               0\n",
      "        Identity-490            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-491           [24, 1152, 4, 4]         221,184\n",
      "     BatchNorm2d-492           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-493           [24, 1152, 4, 4]               0\n",
      "       ZeroPad2d-494           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-495           [24, 1152, 4, 4]          28,800\n",
      "     BatchNorm2d-496           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-497           [24, 1152, 4, 4]               0\n",
      "        Identity-498           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-499             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-500             [24, 48, 1, 1]               0\n",
      "        Identity-501             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-502           [24, 1152, 1, 1]          56,448\n",
      "        Identity-503           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-504            [24, 192, 4, 4]         221,184\n",
      "     BatchNorm2d-505            [24, 192, 4, 4]             384\n",
      "     MBConvBlock-506            [24, 192, 4, 4]               0\n",
      "        Identity-507            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-508           [24, 1152, 4, 4]         221,184\n",
      "     BatchNorm2d-509           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-510           [24, 1152, 4, 4]               0\n",
      "       ZeroPad2d-511           [24, 1152, 6, 6]               0\n",
      "Conv2dStaticSamePadding-512           [24, 1152, 4, 4]          10,368\n",
      "     BatchNorm2d-513           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-514           [24, 1152, 4, 4]               0\n",
      "        Identity-515           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-516             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-517             [24, 48, 1, 1]               0\n",
      "        Identity-518             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-519           [24, 1152, 1, 1]          56,448\n",
      "        Identity-520           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-521            [24, 320, 4, 4]         368,640\n",
      "     BatchNorm2d-522            [24, 320, 4, 4]             640\n",
      "     MBConvBlock-523            [24, 320, 4, 4]               0\n",
      "        Identity-524            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-525            [24, 672, 8, 8]          75,264\n",
      "     BatchNorm2d-526            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-527            [24, 672, 8, 8]               0\n",
      "       ZeroPad2d-528          [24, 672, 11, 11]               0\n",
      "Conv2dStaticSamePadding-529            [24, 672, 4, 4]          16,800\n",
      "     BatchNorm2d-530            [24, 672, 4, 4]           1,344\n",
      "MemoryEfficientSwish-531            [24, 672, 4, 4]               0\n",
      "        Identity-532            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-533             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-534             [24, 28, 1, 1]               0\n",
      "        Identity-535             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-536            [24, 672, 1, 1]          19,488\n",
      "        Identity-537            [24, 672, 4, 4]               0\n",
      "Conv2dStaticSamePadding-538            [24, 192, 4, 4]         129,024\n",
      "     BatchNorm2d-539            [24, 192, 4, 4]             384\n",
      "     MBConvBlock-540            [24, 192, 4, 4]               0\n",
      "        Identity-541            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-542           [24, 1152, 4, 4]         221,184\n",
      "     BatchNorm2d-543           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-544           [24, 1152, 4, 4]               0\n",
      "       ZeroPad2d-545           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-546           [24, 1152, 4, 4]          28,800\n",
      "     BatchNorm2d-547           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-548           [24, 1152, 4, 4]               0\n",
      "        Identity-549           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-550             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-551             [24, 48, 1, 1]               0\n",
      "        Identity-552             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-553           [24, 1152, 1, 1]          56,448\n",
      "        Identity-554           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-555            [24, 192, 4, 4]         221,184\n",
      "     BatchNorm2d-556            [24, 192, 4, 4]             384\n",
      "     MBConvBlock-557            [24, 192, 4, 4]               0\n",
      "        Identity-558            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-559           [24, 1152, 4, 4]         221,184\n",
      "     BatchNorm2d-560           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-561           [24, 1152, 4, 4]               0\n",
      "       ZeroPad2d-562           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-563           [24, 1152, 4, 4]          28,800\n",
      "     BatchNorm2d-564           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-565           [24, 1152, 4, 4]               0\n",
      "        Identity-566           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-567             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-568             [24, 48, 1, 1]               0\n",
      "        Identity-569             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-570           [24, 1152, 1, 1]          56,448\n",
      "        Identity-571           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-572            [24, 192, 4, 4]         221,184\n",
      "     BatchNorm2d-573            [24, 192, 4, 4]             384\n",
      "     MBConvBlock-574            [24, 192, 4, 4]               0\n",
      "        Identity-575            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-576           [24, 1152, 4, 4]         221,184\n",
      "     BatchNorm2d-577           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-578           [24, 1152, 4, 4]               0\n",
      "       ZeroPad2d-579           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-580           [24, 1152, 4, 4]          28,800\n",
      "     BatchNorm2d-581           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-582           [24, 1152, 4, 4]               0\n",
      "        Identity-583           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-584             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-585             [24, 48, 1, 1]               0\n",
      "        Identity-586             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-587           [24, 1152, 1, 1]          56,448\n",
      "        Identity-588           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-589            [24, 192, 4, 4]         221,184\n",
      "     BatchNorm2d-590            [24, 192, 4, 4]             384\n",
      "     MBConvBlock-591            [24, 192, 4, 4]               0\n",
      "        Identity-592            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-593           [24, 1152, 4, 4]         221,184\n",
      "     BatchNorm2d-594           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-595           [24, 1152, 4, 4]               0\n",
      "       ZeroPad2d-596           [24, 1152, 6, 6]               0\n",
      "Conv2dStaticSamePadding-597           [24, 1152, 4, 4]          10,368\n",
      "     BatchNorm2d-598           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-599           [24, 1152, 4, 4]               0\n",
      "        Identity-600           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-601             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-602             [24, 48, 1, 1]               0\n",
      "        Identity-603             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-604           [24, 1152, 1, 1]          56,448\n",
      "        Identity-605           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-606            [24, 320, 4, 4]         368,640\n",
      "     BatchNorm2d-607            [24, 320, 4, 4]             640\n",
      "     MBConvBlock-608            [24, 320, 4, 4]               0\n",
      "     BatchNorm2d-609            [24, 320, 4, 4]             640\n",
      "          Conv2d-610             [24, 80, 4, 4]          25,680\n",
      "          Conv2d-611             [24, 80, 4, 4]          25,680\n",
      "         Softmax-612               [24, 16, 16]               0\n",
      "          Conv2d-613            [24, 320, 4, 4]         102,720\n",
      "            SELU-614            [24, 320, 4, 4]               0\n",
      "            SELU-615            [24, 320, 4, 4]               0\n",
      "CrossModalAttention-616            [24, 320, 4, 4]               0\n",
      "          Conv2d-617            [24, 256, 4, 4]          82,176\n",
      "          Linear-618              [24, 4, 1024]       1,049,600\n",
      "         Dropout-619              [24, 4, 1024]               0\n",
      "       LayerNorm-620              [24, 4, 1024]           2,048\n",
      "          Linear-621               [24, 4, 576]         589,824\n",
      "         Softmax-622              [24, 3, 4, 4]               0\n",
      "          Linear-623              [24, 4, 1024]         197,632\n",
      "         Dropout-624              [24, 4, 1024]               0\n",
      "       Attention-625              [24, 4, 1024]               0\n",
      "         PreNorm-626              [24, 4, 1024]               0\n",
      "       LayerNorm-627              [24, 4, 1024]           2,048\n",
      "          Linear-628              [24, 4, 2048]       2,099,200\n",
      "            GELU-629              [24, 4, 2048]               0\n",
      "         Dropout-630              [24, 4, 2048]               0\n",
      "          Linear-631              [24, 4, 1024]       2,098,176\n",
      "         Dropout-632              [24, 4, 1024]               0\n",
      "     FeedForward-633              [24, 4, 1024]               0\n",
      "         PreNorm-634              [24, 4, 1024]               0\n",
      "       LayerNorm-635              [24, 4, 1024]           2,048\n",
      "          Linear-636               [24, 4, 576]         589,824\n",
      "         Softmax-637              [24, 3, 4, 4]               0\n",
      "          Linear-638              [24, 4, 1024]         197,632\n",
      "         Dropout-639              [24, 4, 1024]               0\n",
      "       Attention-640              [24, 4, 1024]               0\n",
      "         PreNorm-641              [24, 4, 1024]               0\n",
      "       LayerNorm-642              [24, 4, 1024]           2,048\n",
      "          Linear-643              [24, 4, 2048]       2,099,200\n",
      "            GELU-644              [24, 4, 2048]               0\n",
      "         Dropout-645              [24, 4, 2048]               0\n",
      "          Linear-646              [24, 4, 1024]       2,098,176\n",
      "         Dropout-647              [24, 4, 1024]               0\n",
      "     FeedForward-648              [24, 4, 1024]               0\n",
      "         PreNorm-649              [24, 4, 1024]               0\n",
      "     Transformer-650              [24, 4, 1024]               0\n",
      "         Dropout-651                 [24, 1024]               0\n",
      "          Linear-652                 [24, 2048]       2,099,200\n",
      "            ReLU-653                 [24, 2048]               0\n",
      "         Dropout-654                 [24, 2048]               0\n",
      "          Linear-655                    [24, 1]           2,049\n",
      "         Sigmoid-656                    [24, 1]               0\n",
      "       ZeroPad2d-657          [24, 3, 129, 129]               0\n",
      "Conv2dStaticSamePadding-658           [24, 32, 64, 64]             864\n",
      "     BatchNorm2d-659           [24, 32, 64, 64]              64\n",
      "MemoryEfficientSwish-660           [24, 32, 64, 64]               0\n",
      "       ZeroPad2d-661           [24, 32, 66, 66]               0\n",
      "Conv2dStaticSamePadding-662           [24, 32, 64, 64]             288\n",
      "     BatchNorm2d-663           [24, 32, 64, 64]              64\n",
      "MemoryEfficientSwish-664           [24, 32, 64, 64]               0\n",
      "        Identity-665             [24, 32, 1, 1]               0\n",
      "Conv2dStaticSamePadding-666              [24, 8, 1, 1]             264\n",
      "MemoryEfficientSwish-667              [24, 8, 1, 1]               0\n",
      "        Identity-668              [24, 8, 1, 1]               0\n",
      "Conv2dStaticSamePadding-669             [24, 32, 1, 1]             288\n",
      "        Identity-670           [24, 32, 64, 64]               0\n",
      "Conv2dStaticSamePadding-671           [24, 16, 64, 64]             512\n",
      "     BatchNorm2d-672           [24, 16, 64, 64]              32\n",
      "     MBConvBlock-673           [24, 16, 64, 64]               0\n",
      "        Identity-674           [24, 16, 64, 64]               0\n",
      "Conv2dStaticSamePadding-675           [24, 96, 64, 64]           1,536\n",
      "     BatchNorm2d-676           [24, 96, 64, 64]             192\n",
      "MemoryEfficientSwish-677           [24, 96, 64, 64]               0\n",
      "       ZeroPad2d-678           [24, 96, 65, 65]               0\n",
      "Conv2dStaticSamePadding-679           [24, 96, 32, 32]             864\n",
      "     BatchNorm2d-680           [24, 96, 32, 32]             192\n",
      "MemoryEfficientSwish-681           [24, 96, 32, 32]               0\n",
      "        Identity-682             [24, 96, 1, 1]               0\n",
      "Conv2dStaticSamePadding-683              [24, 4, 1, 1]             388\n",
      "MemoryEfficientSwish-684              [24, 4, 1, 1]               0\n",
      "        Identity-685              [24, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-686             [24, 96, 1, 1]             480\n",
      "        Identity-687           [24, 96, 32, 32]               0\n",
      "Conv2dStaticSamePadding-688           [24, 24, 32, 32]           2,304\n",
      "     BatchNorm2d-689           [24, 24, 32, 32]              48\n",
      "     MBConvBlock-690           [24, 24, 32, 32]               0\n",
      "        Identity-691           [24, 24, 32, 32]               0\n",
      "Conv2dStaticSamePadding-692          [24, 144, 32, 32]           3,456\n",
      "     BatchNorm2d-693          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-694          [24, 144, 32, 32]               0\n",
      "       ZeroPad2d-695          [24, 144, 34, 34]               0\n",
      "Conv2dStaticSamePadding-696          [24, 144, 32, 32]           1,296\n",
      "     BatchNorm2d-697          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-698          [24, 144, 32, 32]               0\n",
      "        Identity-699            [24, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-700              [24, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-701              [24, 6, 1, 1]               0\n",
      "        Identity-702              [24, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-703            [24, 144, 1, 1]           1,008\n",
      "        Identity-704          [24, 144, 32, 32]               0\n",
      "Conv2dStaticSamePadding-705           [24, 24, 32, 32]           3,456\n",
      "     BatchNorm2d-706           [24, 24, 32, 32]              48\n",
      "     MBConvBlock-707           [24, 24, 32, 32]               0\n",
      "        Identity-708           [24, 24, 32, 32]               0\n",
      "Conv2dStaticSamePadding-709          [24, 144, 32, 32]           3,456\n",
      "     BatchNorm2d-710          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-711          [24, 144, 32, 32]               0\n",
      "       ZeroPad2d-712          [24, 144, 35, 35]               0\n",
      "Conv2dStaticSamePadding-713          [24, 144, 16, 16]           3,600\n",
      "     BatchNorm2d-714          [24, 144, 16, 16]             288\n",
      "MemoryEfficientSwish-715          [24, 144, 16, 16]               0\n",
      "        Identity-716            [24, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-717              [24, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-718              [24, 6, 1, 1]               0\n",
      "        Identity-719              [24, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-720            [24, 144, 1, 1]           1,008\n",
      "        Identity-721          [24, 144, 16, 16]               0\n",
      "Conv2dStaticSamePadding-722           [24, 40, 16, 16]           5,760\n",
      "     BatchNorm2d-723           [24, 40, 16, 16]              80\n",
      "     MBConvBlock-724           [24, 40, 16, 16]               0\n",
      "        Identity-725           [24, 40, 16, 16]               0\n",
      "Conv2dStaticSamePadding-726          [24, 240, 16, 16]           9,600\n",
      "     BatchNorm2d-727          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-728          [24, 240, 16, 16]               0\n",
      "       ZeroPad2d-729          [24, 240, 20, 20]               0\n",
      "Conv2dStaticSamePadding-730          [24, 240, 16, 16]           6,000\n",
      "     BatchNorm2d-731          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-732          [24, 240, 16, 16]               0\n",
      "        Identity-733            [24, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-734             [24, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-735             [24, 10, 1, 1]               0\n",
      "        Identity-736             [24, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-737            [24, 240, 1, 1]           2,640\n",
      "        Identity-738          [24, 240, 16, 16]               0\n",
      "Conv2dStaticSamePadding-739           [24, 40, 16, 16]           9,600\n",
      "     BatchNorm2d-740           [24, 40, 16, 16]              80\n",
      "     MBConvBlock-741           [24, 40, 16, 16]               0\n",
      "          Conv2d-742           [24, 40, 16, 16]           1,640\n",
      "          Conv2d-743           [24, 40, 16, 16]           1,640\n",
      "          Conv2d-744           [24, 40, 16, 16]           1,640\n",
      "          Conv2d-745           [24, 40, 16, 16]          14,440\n",
      "     BatchNorm2d-746           [24, 40, 16, 16]              80\n",
      "       LeakyReLU-747           [24, 40, 16, 16]               0\n",
      "MultiHeadedAttention-748           [24, 40, 16, 16]               0\n",
      "          Conv2d-749           [24, 40, 16, 16]          14,440\n",
      "     BatchNorm2d-750           [24, 40, 16, 16]              80\n",
      "       LeakyReLU-751           [24, 40, 16, 16]               0\n",
      "          Conv2d-752           [24, 40, 16, 16]          14,440\n",
      "     BatchNorm2d-753           [24, 40, 16, 16]              80\n",
      "       LeakyReLU-754           [24, 40, 16, 16]               0\n",
      "   FeedForward2D-755           [24, 40, 16, 16]               0\n",
      "TransformerBlock-756           [24, 40, 16, 16]               0\n",
      "      PatchTrans-757           [24, 40, 16, 16]               0\n",
      "          Conv2d-758           [24, 40, 16, 16]           1,640\n",
      "          Conv2d-759           [24, 40, 16, 16]           1,640\n",
      "          Conv2d-760           [24, 40, 16, 16]           1,640\n",
      "          Conv2d-761           [24, 40, 16, 16]          14,440\n",
      "     BatchNorm2d-762           [24, 40, 16, 16]              80\n",
      "       LeakyReLU-763           [24, 40, 16, 16]               0\n",
      "MultiHeadedAttention-764           [24, 40, 16, 16]               0\n",
      "          Conv2d-765           [24, 40, 16, 16]          14,440\n",
      "     BatchNorm2d-766           [24, 40, 16, 16]              80\n",
      "       LeakyReLU-767           [24, 40, 16, 16]               0\n",
      "          Conv2d-768           [24, 40, 16, 16]          14,440\n",
      "     BatchNorm2d-769           [24, 40, 16, 16]              80\n",
      "       LeakyReLU-770           [24, 40, 16, 16]               0\n",
      "   FeedForward2D-771           [24, 40, 16, 16]               0\n",
      "TransformerBlock-772           [24, 40, 16, 16]               0\n",
      "      PatchTrans-773           [24, 40, 16, 16]               0\n",
      "       ZeroPad2d-774          [24, 1, 129, 129]               0\n",
      "Conv2dStaticSamePadding-775           [24, 32, 64, 64]             288\n",
      "     BatchNorm2d-776           [24, 32, 64, 64]              64\n",
      "MemoryEfficientSwish-777           [24, 32, 64, 64]               0\n",
      "       ZeroPad2d-778           [24, 32, 66, 66]               0\n",
      "Conv2dStaticSamePadding-779           [24, 32, 64, 64]             288\n",
      "     BatchNorm2d-780           [24, 32, 64, 64]              64\n",
      "MemoryEfficientSwish-781           [24, 32, 64, 64]               0\n",
      "        Identity-782             [24, 32, 1, 1]               0\n",
      "Conv2dStaticSamePadding-783              [24, 8, 1, 1]             264\n",
      "MemoryEfficientSwish-784              [24, 8, 1, 1]               0\n",
      "        Identity-785              [24, 8, 1, 1]               0\n",
      "Conv2dStaticSamePadding-786             [24, 32, 1, 1]             288\n",
      "        Identity-787           [24, 32, 64, 64]               0\n",
      "Conv2dStaticSamePadding-788           [24, 16, 64, 64]             512\n",
      "     BatchNorm2d-789           [24, 16, 64, 64]              32\n",
      "     MBConvBlock-790           [24, 16, 64, 64]               0\n",
      "        Identity-791           [24, 16, 64, 64]               0\n",
      "Conv2dStaticSamePadding-792           [24, 96, 64, 64]           1,536\n",
      "     BatchNorm2d-793           [24, 96, 64, 64]             192\n",
      "MemoryEfficientSwish-794           [24, 96, 64, 64]               0\n",
      "       ZeroPad2d-795           [24, 96, 65, 65]               0\n",
      "Conv2dStaticSamePadding-796           [24, 96, 32, 32]             864\n",
      "     BatchNorm2d-797           [24, 96, 32, 32]             192\n",
      "MemoryEfficientSwish-798           [24, 96, 32, 32]               0\n",
      "        Identity-799             [24, 96, 1, 1]               0\n",
      "Conv2dStaticSamePadding-800              [24, 4, 1, 1]             388\n",
      "MemoryEfficientSwish-801              [24, 4, 1, 1]               0\n",
      "        Identity-802              [24, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-803             [24, 96, 1, 1]             480\n",
      "        Identity-804           [24, 96, 32, 32]               0\n",
      "Conv2dStaticSamePadding-805           [24, 24, 32, 32]           2,304\n",
      "     BatchNorm2d-806           [24, 24, 32, 32]              48\n",
      "     MBConvBlock-807           [24, 24, 32, 32]               0\n",
      "        Identity-808           [24, 24, 32, 32]               0\n",
      "Conv2dStaticSamePadding-809          [24, 144, 32, 32]           3,456\n",
      "     BatchNorm2d-810          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-811          [24, 144, 32, 32]               0\n",
      "       ZeroPad2d-812          [24, 144, 34, 34]               0\n",
      "Conv2dStaticSamePadding-813          [24, 144, 32, 32]           1,296\n",
      "     BatchNorm2d-814          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-815          [24, 144, 32, 32]               0\n",
      "        Identity-816            [24, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-817              [24, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-818              [24, 6, 1, 1]               0\n",
      "        Identity-819              [24, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-820            [24, 144, 1, 1]           1,008\n",
      "        Identity-821          [24, 144, 32, 32]               0\n",
      "Conv2dStaticSamePadding-822           [24, 24, 32, 32]           3,456\n",
      "     BatchNorm2d-823           [24, 24, 32, 32]              48\n",
      "     MBConvBlock-824           [24, 24, 32, 32]               0\n",
      "        Identity-825           [24, 24, 32, 32]               0\n",
      "Conv2dStaticSamePadding-826          [24, 144, 32, 32]           3,456\n",
      "     BatchNorm2d-827          [24, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-828          [24, 144, 32, 32]               0\n",
      "       ZeroPad2d-829          [24, 144, 35, 35]               0\n",
      "Conv2dStaticSamePadding-830          [24, 144, 16, 16]           3,600\n",
      "     BatchNorm2d-831          [24, 144, 16, 16]             288\n",
      "MemoryEfficientSwish-832          [24, 144, 16, 16]               0\n",
      "        Identity-833            [24, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-834              [24, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-835              [24, 6, 1, 1]               0\n",
      "        Identity-836              [24, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-837            [24, 144, 1, 1]           1,008\n",
      "        Identity-838          [24, 144, 16, 16]               0\n",
      "Conv2dStaticSamePadding-839           [24, 40, 16, 16]           5,760\n",
      "     BatchNorm2d-840           [24, 40, 16, 16]              80\n",
      "     MBConvBlock-841           [24, 40, 16, 16]               0\n",
      "        Identity-842           [24, 40, 16, 16]               0\n",
      "Conv2dStaticSamePadding-843          [24, 240, 16, 16]           9,600\n",
      "     BatchNorm2d-844          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-845          [24, 240, 16, 16]               0\n",
      "       ZeroPad2d-846          [24, 240, 20, 20]               0\n",
      "Conv2dStaticSamePadding-847          [24, 240, 16, 16]           6,000\n",
      "     BatchNorm2d-848          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-849          [24, 240, 16, 16]               0\n",
      "        Identity-850            [24, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-851             [24, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-852             [24, 10, 1, 1]               0\n",
      "        Identity-853             [24, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-854            [24, 240, 1, 1]           2,640\n",
      "        Identity-855          [24, 240, 16, 16]               0\n",
      "Conv2dStaticSamePadding-856           [24, 40, 16, 16]           9,600\n",
      "     BatchNorm2d-857           [24, 40, 16, 16]              80\n",
      "     MBConvBlock-858           [24, 40, 16, 16]               0\n",
      "        Identity-859           [24, 40, 16, 16]               0\n",
      "Conv2dStaticSamePadding-860          [24, 240, 16, 16]           9,600\n",
      "     BatchNorm2d-861          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-862          [24, 240, 16, 16]               0\n",
      "       ZeroPad2d-863          [24, 240, 17, 17]               0\n",
      "Conv2dStaticSamePadding-864            [24, 240, 8, 8]           2,160\n",
      "     BatchNorm2d-865            [24, 240, 8, 8]             480\n",
      "MemoryEfficientSwish-866            [24, 240, 8, 8]               0\n",
      "        Identity-867            [24, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-868             [24, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-869             [24, 10, 1, 1]               0\n",
      "        Identity-870             [24, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-871            [24, 240, 1, 1]           2,640\n",
      "        Identity-872            [24, 240, 8, 8]               0\n",
      "Conv2dStaticSamePadding-873             [24, 80, 8, 8]          19,200\n",
      "     BatchNorm2d-874             [24, 80, 8, 8]             160\n",
      "     MBConvBlock-875             [24, 80, 8, 8]               0\n",
      "        Identity-876             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-877            [24, 480, 8, 8]          38,400\n",
      "     BatchNorm2d-878            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-879            [24, 480, 8, 8]               0\n",
      "       ZeroPad2d-880          [24, 480, 10, 10]               0\n",
      "Conv2dStaticSamePadding-881            [24, 480, 8, 8]           4,320\n",
      "     BatchNorm2d-882            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-883            [24, 480, 8, 8]               0\n",
      "        Identity-884            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-885             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-886             [24, 20, 1, 1]               0\n",
      "        Identity-887             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-888            [24, 480, 1, 1]          10,080\n",
      "        Identity-889            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-890             [24, 80, 8, 8]          38,400\n",
      "     BatchNorm2d-891             [24, 80, 8, 8]             160\n",
      "     MBConvBlock-892             [24, 80, 8, 8]               0\n",
      "        Identity-893             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-894            [24, 480, 8, 8]          38,400\n",
      "     BatchNorm2d-895            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-896            [24, 480, 8, 8]               0\n",
      "       ZeroPad2d-897          [24, 480, 10, 10]               0\n",
      "Conv2dStaticSamePadding-898            [24, 480, 8, 8]           4,320\n",
      "     BatchNorm2d-899            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-900            [24, 480, 8, 8]               0\n",
      "        Identity-901            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-902             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-903             [24, 20, 1, 1]               0\n",
      "        Identity-904             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-905            [24, 480, 1, 1]          10,080\n",
      "        Identity-906            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-907             [24, 80, 8, 8]          38,400\n",
      "     BatchNorm2d-908             [24, 80, 8, 8]             160\n",
      "     MBConvBlock-909             [24, 80, 8, 8]               0\n",
      "        Identity-910             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-911            [24, 480, 8, 8]          38,400\n",
      "     BatchNorm2d-912            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-913            [24, 480, 8, 8]               0\n",
      "       ZeroPad2d-914          [24, 480, 12, 12]               0\n",
      "Conv2dStaticSamePadding-915            [24, 480, 8, 8]          12,000\n",
      "     BatchNorm2d-916            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-917            [24, 480, 8, 8]               0\n",
      "        Identity-918            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-919             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-920             [24, 20, 1, 1]               0\n",
      "        Identity-921             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-922            [24, 480, 1, 1]          10,080\n",
      "        Identity-923            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-924            [24, 112, 8, 8]          53,760\n",
      "     BatchNorm2d-925            [24, 112, 8, 8]             224\n",
      "     MBConvBlock-926            [24, 112, 8, 8]               0\n",
      "        Identity-927            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-928            [24, 672, 8, 8]          75,264\n",
      "     BatchNorm2d-929            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-930            [24, 672, 8, 8]               0\n",
      "       ZeroPad2d-931          [24, 672, 12, 12]               0\n",
      "Conv2dStaticSamePadding-932            [24, 672, 8, 8]          16,800\n",
      "     BatchNorm2d-933            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-934            [24, 672, 8, 8]               0\n",
      "        Identity-935            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-936             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-937             [24, 28, 1, 1]               0\n",
      "        Identity-938             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-939            [24, 672, 1, 1]          19,488\n",
      "        Identity-940            [24, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-941            [24, 112, 8, 8]          75,264\n",
      "     BatchNorm2d-942            [24, 112, 8, 8]             224\n",
      "     MBConvBlock-943            [24, 112, 8, 8]               0\n",
      "        Identity-944            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-945            [24, 672, 8, 8]          75,264\n",
      "     BatchNorm2d-946            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-947            [24, 672, 8, 8]               0\n",
      "       ZeroPad2d-948          [24, 672, 12, 12]               0\n",
      "Conv2dStaticSamePadding-949            [24, 672, 8, 8]          16,800\n",
      "     BatchNorm2d-950            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-951            [24, 672, 8, 8]               0\n",
      "        Identity-952            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-953             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-954             [24, 28, 1, 1]               0\n",
      "        Identity-955             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-956            [24, 672, 1, 1]          19,488\n",
      "        Identity-957            [24, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-958            [24, 112, 8, 8]          75,264\n",
      "     BatchNorm2d-959            [24, 112, 8, 8]             224\n",
      "     MBConvBlock-960            [24, 112, 8, 8]               0\n",
      "        Identity-961           [24, 40, 16, 16]               0\n",
      "Conv2dStaticSamePadding-962          [24, 240, 16, 16]           9,600\n",
      "     BatchNorm2d-963          [24, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-964          [24, 240, 16, 16]               0\n",
      "       ZeroPad2d-965          [24, 240, 17, 17]               0\n",
      "Conv2dStaticSamePadding-966            [24, 240, 8, 8]           2,160\n",
      "     BatchNorm2d-967            [24, 240, 8, 8]             480\n",
      "MemoryEfficientSwish-968            [24, 240, 8, 8]               0\n",
      "        Identity-969            [24, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-970             [24, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-971             [24, 10, 1, 1]               0\n",
      "        Identity-972             [24, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-973            [24, 240, 1, 1]           2,640\n",
      "        Identity-974            [24, 240, 8, 8]               0\n",
      "Conv2dStaticSamePadding-975             [24, 80, 8, 8]          19,200\n",
      "     BatchNorm2d-976             [24, 80, 8, 8]             160\n",
      "     MBConvBlock-977             [24, 80, 8, 8]               0\n",
      "        Identity-978             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-979            [24, 480, 8, 8]          38,400\n",
      "     BatchNorm2d-980            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-981            [24, 480, 8, 8]               0\n",
      "       ZeroPad2d-982          [24, 480, 10, 10]               0\n",
      "Conv2dStaticSamePadding-983            [24, 480, 8, 8]           4,320\n",
      "     BatchNorm2d-984            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-985            [24, 480, 8, 8]               0\n",
      "        Identity-986            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-987             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-988             [24, 20, 1, 1]               0\n",
      "        Identity-989             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-990            [24, 480, 1, 1]          10,080\n",
      "        Identity-991            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-992             [24, 80, 8, 8]          38,400\n",
      "     BatchNorm2d-993             [24, 80, 8, 8]             160\n",
      "     MBConvBlock-994             [24, 80, 8, 8]               0\n",
      "        Identity-995             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-996            [24, 480, 8, 8]          38,400\n",
      "     BatchNorm2d-997            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-998            [24, 480, 8, 8]               0\n",
      "       ZeroPad2d-999          [24, 480, 10, 10]               0\n",
      "Conv2dStaticSamePadding-1000            [24, 480, 8, 8]           4,320\n",
      "    BatchNorm2d-1001            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-1002            [24, 480, 8, 8]               0\n",
      "       Identity-1003            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1004             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-1005             [24, 20, 1, 1]               0\n",
      "       Identity-1006             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1007            [24, 480, 1, 1]          10,080\n",
      "       Identity-1008            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1009             [24, 80, 8, 8]          38,400\n",
      "    BatchNorm2d-1010             [24, 80, 8, 8]             160\n",
      "    MBConvBlock-1011             [24, 80, 8, 8]               0\n",
      "       Identity-1012             [24, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1013            [24, 480, 8, 8]          38,400\n",
      "    BatchNorm2d-1014            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-1015            [24, 480, 8, 8]               0\n",
      "      ZeroPad2d-1016          [24, 480, 12, 12]               0\n",
      "Conv2dStaticSamePadding-1017            [24, 480, 8, 8]          12,000\n",
      "    BatchNorm2d-1018            [24, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-1019            [24, 480, 8, 8]               0\n",
      "       Identity-1020            [24, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1021             [24, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-1022             [24, 20, 1, 1]               0\n",
      "       Identity-1023             [24, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1024            [24, 480, 1, 1]          10,080\n",
      "       Identity-1025            [24, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1026            [24, 112, 8, 8]          53,760\n",
      "    BatchNorm2d-1027            [24, 112, 8, 8]             224\n",
      "    MBConvBlock-1028            [24, 112, 8, 8]               0\n",
      "       Identity-1029            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1030            [24, 672, 8, 8]          75,264\n",
      "    BatchNorm2d-1031            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-1032            [24, 672, 8, 8]               0\n",
      "      ZeroPad2d-1033          [24, 672, 12, 12]               0\n",
      "Conv2dStaticSamePadding-1034            [24, 672, 8, 8]          16,800\n",
      "    BatchNorm2d-1035            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-1036            [24, 672, 8, 8]               0\n",
      "       Identity-1037            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1038             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-1039             [24, 28, 1, 1]               0\n",
      "       Identity-1040             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1041            [24, 672, 1, 1]          19,488\n",
      "       Identity-1042            [24, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1043            [24, 112, 8, 8]          75,264\n",
      "    BatchNorm2d-1044            [24, 112, 8, 8]             224\n",
      "    MBConvBlock-1045            [24, 112, 8, 8]               0\n",
      "       Identity-1046            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1047            [24, 672, 8, 8]          75,264\n",
      "    BatchNorm2d-1048            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-1049            [24, 672, 8, 8]               0\n",
      "      ZeroPad2d-1050          [24, 672, 12, 12]               0\n",
      "Conv2dStaticSamePadding-1051            [24, 672, 8, 8]          16,800\n",
      "    BatchNorm2d-1052            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-1053            [24, 672, 8, 8]               0\n",
      "       Identity-1054            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1055             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-1056             [24, 28, 1, 1]               0\n",
      "       Identity-1057             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1058            [24, 672, 1, 1]          19,488\n",
      "       Identity-1059            [24, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1060            [24, 112, 8, 8]          75,264\n",
      "    BatchNorm2d-1061            [24, 112, 8, 8]             224\n",
      "    MBConvBlock-1062            [24, 112, 8, 8]               0\n",
      "         Conv2d-1063            [24, 112, 8, 8]          12,656\n",
      "         Conv2d-1064            [24, 112, 8, 8]          12,656\n",
      "         Conv2d-1065            [24, 112, 8, 8]          12,656\n",
      "         Conv2d-1066            [24, 112, 8, 8]         113,008\n",
      "    BatchNorm2d-1067            [24, 112, 8, 8]             224\n",
      "      LeakyReLU-1068            [24, 112, 8, 8]               0\n",
      "MultiHeadedAttentionv2-1069            [24, 112, 8, 8]               0\n",
      "         Conv2d-1070            [24, 112, 8, 8]         113,008\n",
      "    BatchNorm2d-1071            [24, 112, 8, 8]             224\n",
      "      LeakyReLU-1072            [24, 112, 8, 8]               0\n",
      "         Conv2d-1073            [24, 112, 8, 8]         113,008\n",
      "    BatchNorm2d-1074            [24, 112, 8, 8]             224\n",
      "      LeakyReLU-1075            [24, 112, 8, 8]               0\n",
      "  FeedForward2D-1076            [24, 112, 8, 8]               0\n",
      "TransformerBlockv2-1077            [24, 112, 8, 8]               0\n",
      "   PatchTransv2-1078            [24, 112, 8, 8]               0\n",
      "         Conv2d-1079            [24, 112, 8, 8]          12,656\n",
      "         Conv2d-1080            [24, 112, 8, 8]          12,656\n",
      "         Conv2d-1081            [24, 112, 8, 8]          12,656\n",
      "         Conv2d-1082            [24, 112, 8, 8]         113,008\n",
      "    BatchNorm2d-1083            [24, 112, 8, 8]             224\n",
      "      LeakyReLU-1084            [24, 112, 8, 8]               0\n",
      "MultiHeadedAttentionv2-1085            [24, 112, 8, 8]               0\n",
      "         Conv2d-1086            [24, 112, 8, 8]         113,008\n",
      "    BatchNorm2d-1087            [24, 112, 8, 8]             224\n",
      "      LeakyReLU-1088            [24, 112, 8, 8]               0\n",
      "         Conv2d-1089            [24, 112, 8, 8]         113,008\n",
      "    BatchNorm2d-1090            [24, 112, 8, 8]             224\n",
      "      LeakyReLU-1091            [24, 112, 8, 8]               0\n",
      "  FeedForward2D-1092            [24, 112, 8, 8]               0\n",
      "TransformerBlockv2-1093            [24, 112, 8, 8]               0\n",
      "   PatchTransv2-1094            [24, 112, 8, 8]               0\n",
      "       Identity-1095            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1096            [24, 672, 8, 8]          75,264\n",
      "    BatchNorm2d-1097            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-1098            [24, 672, 8, 8]               0\n",
      "      ZeroPad2d-1099          [24, 672, 11, 11]               0\n",
      "Conv2dStaticSamePadding-1100            [24, 672, 4, 4]          16,800\n",
      "    BatchNorm2d-1101            [24, 672, 4, 4]           1,344\n",
      "MemoryEfficientSwish-1102            [24, 672, 4, 4]               0\n",
      "       Identity-1103            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1104             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-1105             [24, 28, 1, 1]               0\n",
      "       Identity-1106             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1107            [24, 672, 1, 1]          19,488\n",
      "       Identity-1108            [24, 672, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1109            [24, 192, 4, 4]         129,024\n",
      "    BatchNorm2d-1110            [24, 192, 4, 4]             384\n",
      "    MBConvBlock-1111            [24, 192, 4, 4]               0\n",
      "       Identity-1112            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1113           [24, 1152, 4, 4]         221,184\n",
      "    BatchNorm2d-1114           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1115           [24, 1152, 4, 4]               0\n",
      "      ZeroPad2d-1116           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1117           [24, 1152, 4, 4]          28,800\n",
      "    BatchNorm2d-1118           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1119           [24, 1152, 4, 4]               0\n",
      "       Identity-1120           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1121             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-1122             [24, 48, 1, 1]               0\n",
      "       Identity-1123             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1124           [24, 1152, 1, 1]          56,448\n",
      "       Identity-1125           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1126            [24, 192, 4, 4]         221,184\n",
      "    BatchNorm2d-1127            [24, 192, 4, 4]             384\n",
      "    MBConvBlock-1128            [24, 192, 4, 4]               0\n",
      "       Identity-1129            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1130           [24, 1152, 4, 4]         221,184\n",
      "    BatchNorm2d-1131           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1132           [24, 1152, 4, 4]               0\n",
      "      ZeroPad2d-1133           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1134           [24, 1152, 4, 4]          28,800\n",
      "    BatchNorm2d-1135           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1136           [24, 1152, 4, 4]               0\n",
      "       Identity-1137           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1138             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-1139             [24, 48, 1, 1]               0\n",
      "       Identity-1140             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1141           [24, 1152, 1, 1]          56,448\n",
      "       Identity-1142           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1143            [24, 192, 4, 4]         221,184\n",
      "    BatchNorm2d-1144            [24, 192, 4, 4]             384\n",
      "    MBConvBlock-1145            [24, 192, 4, 4]               0\n",
      "       Identity-1146            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1147           [24, 1152, 4, 4]         221,184\n",
      "    BatchNorm2d-1148           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1149           [24, 1152, 4, 4]               0\n",
      "      ZeroPad2d-1150           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1151           [24, 1152, 4, 4]          28,800\n",
      "    BatchNorm2d-1152           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1153           [24, 1152, 4, 4]               0\n",
      "       Identity-1154           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1155             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-1156             [24, 48, 1, 1]               0\n",
      "       Identity-1157             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1158           [24, 1152, 1, 1]          56,448\n",
      "       Identity-1159           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1160            [24, 192, 4, 4]         221,184\n",
      "    BatchNorm2d-1161            [24, 192, 4, 4]             384\n",
      "    MBConvBlock-1162            [24, 192, 4, 4]               0\n",
      "       Identity-1163            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1164           [24, 1152, 4, 4]         221,184\n",
      "    BatchNorm2d-1165           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1166           [24, 1152, 4, 4]               0\n",
      "      ZeroPad2d-1167           [24, 1152, 6, 6]               0\n",
      "Conv2dStaticSamePadding-1168           [24, 1152, 4, 4]          10,368\n",
      "    BatchNorm2d-1169           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1170           [24, 1152, 4, 4]               0\n",
      "       Identity-1171           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1172             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-1173             [24, 48, 1, 1]               0\n",
      "       Identity-1174             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1175           [24, 1152, 1, 1]          56,448\n",
      "       Identity-1176           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1177            [24, 320, 4, 4]         368,640\n",
      "    BatchNorm2d-1178            [24, 320, 4, 4]             640\n",
      "    MBConvBlock-1179            [24, 320, 4, 4]               0\n",
      "       Identity-1180            [24, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1181            [24, 672, 8, 8]          75,264\n",
      "    BatchNorm2d-1182            [24, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-1183            [24, 672, 8, 8]               0\n",
      "      ZeroPad2d-1184          [24, 672, 11, 11]               0\n",
      "Conv2dStaticSamePadding-1185            [24, 672, 4, 4]          16,800\n",
      "    BatchNorm2d-1186            [24, 672, 4, 4]           1,344\n",
      "MemoryEfficientSwish-1187            [24, 672, 4, 4]               0\n",
      "       Identity-1188            [24, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1189             [24, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-1190             [24, 28, 1, 1]               0\n",
      "       Identity-1191             [24, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1192            [24, 672, 1, 1]          19,488\n",
      "       Identity-1193            [24, 672, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1194            [24, 192, 4, 4]         129,024\n",
      "    BatchNorm2d-1195            [24, 192, 4, 4]             384\n",
      "    MBConvBlock-1196            [24, 192, 4, 4]               0\n",
      "       Identity-1197            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1198           [24, 1152, 4, 4]         221,184\n",
      "    BatchNorm2d-1199           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1200           [24, 1152, 4, 4]               0\n",
      "      ZeroPad2d-1201           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1202           [24, 1152, 4, 4]          28,800\n",
      "    BatchNorm2d-1203           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1204           [24, 1152, 4, 4]               0\n",
      "       Identity-1205           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1206             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-1207             [24, 48, 1, 1]               0\n",
      "       Identity-1208             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1209           [24, 1152, 1, 1]          56,448\n",
      "       Identity-1210           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1211            [24, 192, 4, 4]         221,184\n",
      "    BatchNorm2d-1212            [24, 192, 4, 4]             384\n",
      "    MBConvBlock-1213            [24, 192, 4, 4]               0\n",
      "       Identity-1214            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1215           [24, 1152, 4, 4]         221,184\n",
      "    BatchNorm2d-1216           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1217           [24, 1152, 4, 4]               0\n",
      "      ZeroPad2d-1218           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1219           [24, 1152, 4, 4]          28,800\n",
      "    BatchNorm2d-1220           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1221           [24, 1152, 4, 4]               0\n",
      "       Identity-1222           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1223             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-1224             [24, 48, 1, 1]               0\n",
      "       Identity-1225             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1226           [24, 1152, 1, 1]          56,448\n",
      "       Identity-1227           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1228            [24, 192, 4, 4]         221,184\n",
      "    BatchNorm2d-1229            [24, 192, 4, 4]             384\n",
      "    MBConvBlock-1230            [24, 192, 4, 4]               0\n",
      "       Identity-1231            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1232           [24, 1152, 4, 4]         221,184\n",
      "    BatchNorm2d-1233           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1234           [24, 1152, 4, 4]               0\n",
      "      ZeroPad2d-1235           [24, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-1236           [24, 1152, 4, 4]          28,800\n",
      "    BatchNorm2d-1237           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1238           [24, 1152, 4, 4]               0\n",
      "       Identity-1239           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1240             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-1241             [24, 48, 1, 1]               0\n",
      "       Identity-1242             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1243           [24, 1152, 1, 1]          56,448\n",
      "       Identity-1244           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1245            [24, 192, 4, 4]         221,184\n",
      "    BatchNorm2d-1246            [24, 192, 4, 4]             384\n",
      "    MBConvBlock-1247            [24, 192, 4, 4]               0\n",
      "       Identity-1248            [24, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1249           [24, 1152, 4, 4]         221,184\n",
      "    BatchNorm2d-1250           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1251           [24, 1152, 4, 4]               0\n",
      "      ZeroPad2d-1252           [24, 1152, 6, 6]               0\n",
      "Conv2dStaticSamePadding-1253           [24, 1152, 4, 4]          10,368\n",
      "    BatchNorm2d-1254           [24, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-1255           [24, 1152, 4, 4]               0\n",
      "       Identity-1256           [24, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1257             [24, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-1258             [24, 48, 1, 1]               0\n",
      "       Identity-1259             [24, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-1260           [24, 1152, 1, 1]          56,448\n",
      "       Identity-1261           [24, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-1262            [24, 320, 4, 4]         368,640\n",
      "    BatchNorm2d-1263            [24, 320, 4, 4]             640\n",
      "    MBConvBlock-1264            [24, 320, 4, 4]               0\n",
      "    BatchNorm2d-1265            [24, 320, 4, 4]             640\n",
      "         Conv2d-1266             [24, 80, 4, 4]          25,680\n",
      "         Conv2d-1267             [24, 80, 4, 4]          25,680\n",
      "        Softmax-1268               [24, 16, 16]               0\n",
      "         Conv2d-1269            [24, 320, 4, 4]         102,720\n",
      "           SELU-1270            [24, 320, 4, 4]               0\n",
      "           SELU-1271            [24, 320, 4, 4]               0\n",
      "CrossModalAttention-1272            [24, 320, 4, 4]               0\n",
      "         Conv2d-1273            [24, 256, 4, 4]          82,176\n",
      "         Linear-1274              [24, 4, 1024]       1,049,600\n",
      "        Dropout-1275              [24, 4, 1024]               0\n",
      "      LayerNorm-1276              [24, 4, 1024]           2,048\n",
      "         Linear-1277               [24, 4, 576]         589,824\n",
      "        Softmax-1278              [24, 3, 4, 4]               0\n",
      "         Linear-1279              [24, 4, 1024]         197,632\n",
      "        Dropout-1280              [24, 4, 1024]               0\n",
      "      Attention-1281              [24, 4, 1024]               0\n",
      "        PreNorm-1282              [24, 4, 1024]               0\n",
      "      LayerNorm-1283              [24, 4, 1024]           2,048\n",
      "         Linear-1284              [24, 4, 2048]       2,099,200\n",
      "           GELU-1285              [24, 4, 2048]               0\n",
      "        Dropout-1286              [24, 4, 2048]               0\n",
      "         Linear-1287              [24, 4, 1024]       2,098,176\n",
      "        Dropout-1288              [24, 4, 1024]               0\n",
      "    FeedForward-1289              [24, 4, 1024]               0\n",
      "        PreNorm-1290              [24, 4, 1024]               0\n",
      "      LayerNorm-1291              [24, 4, 1024]           2,048\n",
      "         Linear-1292               [24, 4, 576]         589,824\n",
      "        Softmax-1293              [24, 3, 4, 4]               0\n",
      "         Linear-1294              [24, 4, 1024]         197,632\n",
      "        Dropout-1295              [24, 4, 1024]               0\n",
      "      Attention-1296              [24, 4, 1024]               0\n",
      "        PreNorm-1297              [24, 4, 1024]               0\n",
      "      LayerNorm-1298              [24, 4, 1024]           2,048\n",
      "         Linear-1299              [24, 4, 2048]       2,099,200\n",
      "           GELU-1300              [24, 4, 2048]               0\n",
      "        Dropout-1301              [24, 4, 2048]               0\n",
      "         Linear-1302              [24, 4, 1024]       2,098,176\n",
      "        Dropout-1303              [24, 4, 1024]               0\n",
      "    FeedForward-1304              [24, 4, 1024]               0\n",
      "        PreNorm-1305              [24, 4, 1024]               0\n",
      "    Transformer-1306              [24, 4, 1024]               0\n",
      "        Dropout-1307                 [24, 1024]               0\n",
      "         Linear-1308                 [24, 2048]       2,099,200\n",
      "           ReLU-1309                 [24, 2048]               0\n",
      "        Dropout-1310                 [24, 2048]               0\n",
      "         Linear-1311                    [24, 1]           2,049\n",
      "        Sigmoid-1312                    [24, 1]               0\n",
      "================================================================\n",
      "Total params: 42,816,178\n",
      "Trainable params: 42,816,178\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 10995116277760.00\n",
      "Forward/backward pass size (MB): 7072.86\n",
      "Params size (MB): 163.33\n",
      "Estimated Total Size (MB): 10995116284996.19\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/anaconda3/envs/phucnp/lib/python3.8/site-packages/torchsummary/torchsummary.py:100: RuntimeWarning: overflow encountered in long_scalars\n",
      "  total_input_size = abs(np.prod(input_size) * batch_size * 4. / (1024 ** 2.))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit.vit import ViT, Transformer\n",
    "from model.vision_transformer.cnn_vit.efficient_vit import EfficientViT\n",
    "from pytorchcv.model_provider import get_model\n",
    "from model.backbone.efficient_net.utils import Conv2dStaticSamePadding\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\" CMA attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, activation=None, ratio=8, cross_value=True, gamma_cma=-1):\n",
    "        super().__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.activation = activation\n",
    "        self.cross_value = cross_value\n",
    "\n",
    "        self.query_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//ratio, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//ratio, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        if gamma_cma == -1:\n",
    "            self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        else:\n",
    "            self.gamma = gamma_cma\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature\n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        proj_query = self.query_conv(x).view(\n",
    "            B, -1, H*W).permute(0, 2, 1)  # B , HW, C\n",
    "        proj_key = self.key_conv(y).view(\n",
    "            B, -1, H*W)  # B X C x (*W*H)\n",
    "        energy = torch.bmm(proj_query, proj_key)  # B, HW, HW\n",
    "        attention = self.softmax(energy)  # BX (N) X (N)\n",
    "        if self.cross_value:\n",
    "            proj_value = self.value_conv(z).view(\n",
    "                B, -1, H*W)  # B , C , HW\n",
    "        else:\n",
    "            proj_value = self.value_conv(z).view(\n",
    "                B, -1, H*W)  # B , C , HW\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(B, C, H, W)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "\n",
    "        if self.activation is not None:\n",
    "            out = self.activation(out)\n",
    "        # print(\"out: \", out.shape)\n",
    "        return out  # , attention\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patchsize, d_model):\n",
    "        super().__init__()\n",
    "        self.patchsize = patchsize\n",
    "        self.query_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.value_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.key_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(d_model),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(\n",
    "            query.size(-1)\n",
    "        )\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        p_val = torch.matmul(p_attn, value)\n",
    "        return p_val, p_attn\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()   # 32, 1280, 8, 8\n",
    "        # print(\"x size:\", x.size())\n",
    "        d_k = c // len(self.patchsize)  # 320\n",
    "        output = []\n",
    "        _query = self.query_embedding(x)\n",
    "        _key = self.key_embedding(x)\n",
    "        _value = self.value_embedding(x)\n",
    "        attentions = []\n",
    "        # print(\"_query: \", _query.shape)\n",
    "        for (width, height), query, key, value in zip(\n",
    "            self.patchsize,\n",
    "            torch.chunk(_query, len(self.patchsize), dim=1),\n",
    "            torch.chunk(_key, len(self.patchsize), dim=1),\n",
    "            torch.chunk(_value, len(self.patchsize), dim=1),\n",
    "        ):\n",
    "            # print('query: ', query.shape)   # (B, )\n",
    "            out_w, out_h = w // width, h // height\n",
    "\n",
    "            # 1) embedding and reshape\n",
    "            query = query.view(b, d_k, out_h, height, out_w, width)\n",
    "            query = (\n",
    "                query.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "            key = key.view(b, d_k, out_h, height, out_w, width)\n",
    "            key = (\n",
    "                key.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "            value = value.view(b, d_k, out_h, height, out_w, width)\n",
    "            value = (\n",
    "                value.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "\n",
    "            y, _ = self.attention(query, key, value)\n",
    "\n",
    "            # 3) \"Concat\" using a view and apply a final linear.\n",
    "            y = y.view(b, out_h, out_w, d_k, height, width)\n",
    "            y = y.permute(0, 3, 1, 4, 2, 5).contiguous().view(b, d_k, h, w)\n",
    "            attentions.append(y)\n",
    "            output.append(y)\n",
    "\n",
    "        output = torch.cat(output, 1)\n",
    "        self_attention = self.output_linear(output)\n",
    "\n",
    "        return self_attention\n",
    "\n",
    "class MultiHeadedAttentionv2(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patchsize, d_model):\n",
    "        super().__init__()\n",
    "        self.patchsize = patchsize\n",
    "        self.query_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.value_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.key_embedding = nn.Conv2d(\n",
    "            d_model, d_model, kernel_size=1, padding=0\n",
    "        )\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(d_model),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(\n",
    "            query.size(-1)\n",
    "        )\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        p_val = torch.matmul(p_attn, value)\n",
    "        return p_val, p_attn\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        b, c, h, w = x.size()   # 32, 1280, 8, 8\n",
    "        # print(\"x size:\", x.size())\n",
    "        d_k = c // len(self.patchsize)  # 320\n",
    "        output = []\n",
    "        _query = self.query_embedding(x)\n",
    "        _key = self.key_embedding(y)\n",
    "        _value = self.value_embedding(y)\n",
    "        attentions = []\n",
    "        # print(\"_query: \", _query.shape)\n",
    "        for (width, height), query, key, value in zip(\n",
    "            self.patchsize,\n",
    "            torch.chunk(_query, len(self.patchsize), dim=1),\n",
    "            torch.chunk(_key, len(self.patchsize), dim=1),\n",
    "            torch.chunk(_value, len(self.patchsize), dim=1),\n",
    "        ):\n",
    "            # print('query: ', query.shape)   # (B, )\n",
    "            out_w, out_h = w // width, h // height\n",
    "\n",
    "            # 1) embedding and reshape\n",
    "            query = query.view(b, d_k, out_h, height, out_w, width)\n",
    "            query = (\n",
    "                query.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "            key = key.view(b, d_k, out_h, height, out_w, width)\n",
    "            key = (\n",
    "                key.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "            value = value.view(b, d_k, out_h, height, out_w, width)\n",
    "            value = (\n",
    "                value.permute(0, 2, 4, 1, 3, 5)\n",
    "                .contiguous()\n",
    "                .view(b, out_h * out_w, d_k * height * width)\n",
    "            )\n",
    "\n",
    "            out, attn = self.attention(query, key, value)\n",
    "\n",
    "            # 3) \"Concat\" using a view and apply a final linear.\n",
    "            out = out.view(b, out_h, out_w, d_k, height, width)\n",
    "            out = out.permute(0, 3, 1, 4, 2, 5).contiguous().view(b, d_k, h, w)\n",
    "            attentions.append(attn)\n",
    "            output.append(out)\n",
    "\n",
    "        output = torch.cat(output, 1)\n",
    "        self_attention = self.output_linear(output)\n",
    "        return self_attention\n",
    "\n",
    "class FeedForward2D(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channel, out_channel, kernel_size=3, padding=2, dilation=2\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class PatchTrans(nn.Module):\n",
    "    def __init__(self, in_channel, in_size, patch_resolution=\"1-2-4-8\"):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "\n",
    "        patchsize = []\n",
    "        reso = map(float, patch_resolution.split(\"-\"))\n",
    "        for r in reso:\n",
    "            patchsize.append((int(in_size//r), int(in_size//r)))\n",
    "        # print(patchsize)\n",
    "        self.transform_ = TransformerBlock(patchsize, in_channel=in_channel)\n",
    "        # print(in_channel)\n",
    "\n",
    "    def forward(self, enc_feat):\n",
    "        output = self.transform_(enc_feat)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patchsize, in_channel=256):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(patchsize, d_model=in_channel)\n",
    "        self.feed_forward = FeedForward2D(\n",
    "            in_channel=in_channel, out_channel=in_channel\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb):\n",
    "        self_attention = self.attention(rgb)\n",
    "        output = rgb + self_attention\n",
    "        output = output + self.feed_forward(output)\n",
    "        return output\n",
    "\n",
    "class TransformerBlockv2(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patchsize, in_channel=256):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttentionv2(patchsize, d_model=in_channel)\n",
    "        self.feed_forward = FeedForward2D(\n",
    "            in_channel=in_channel, out_channel=in_channel\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb, freq):\n",
    "        self_attention = self.attention(rgb, freq)\n",
    "        output = rgb + self_attention\n",
    "        output = output + self.feed_forward(output)\n",
    "        return output\n",
    "\n",
    "class PatchTransv2(nn.Module):\n",
    "    def __init__(self, in_channel, in_size, patch_resolution=\"1-2-4-8\"):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "\n",
    "        patchsize = []\n",
    "        reso = map(float, patch_resolution.split(\"-\"))\n",
    "        for r in reso:\n",
    "            patchsize.append((int(in_size//r), int(in_size//r)))\n",
    "        # print(patchsize)\n",
    "        self.transform_ = TransformerBlockv2(patchsize, in_channel=in_channel)\n",
    "        # print(in_channel)\n",
    "\n",
    "    def forward(self, rgb_fea, freq_fea):\n",
    "        output = self.transform_(rgb_fea, freq_fea)\n",
    "        return output\n",
    "\n",
    "class PairwiseDualPatchCNNCMAViT(nn.Module):\n",
    "    def __init__(self, image_size=224, num_classes=1, depth_block4=2, \\\n",
    "                backbone='xception_net', pretrained=True, unfreeze_blocks=-1, \\\n",
    "                normalize_ifft='batchnorm',\\\n",
    "                act='none',\\\n",
    "                init_type=\"xavier_uniform\", \\\n",
    "                gamma_cma=-1, flatten_type='patch', patch_size=2, \\\n",
    "                dim=1024, depth_vit=2, heads=3, dim_head=64, dropout=0.15, emb_dropout=0.15, mlp_dim=2048, dropout_in_mlp=0.0, \\\n",
    "                classifier='mlp', in_vit_channels=64):  \n",
    "        super(PairwiseDualPatchCNNCMAViT, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.depth_block4 = depth_block4\n",
    "\n",
    "        self.depth_vit = depth_vit\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.flatten_type = flatten_type\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.features_size = {\n",
    "            'efficient_net': (1280, 8, 8),\n",
    "            'xception_net': (2048, 8, 8),\n",
    "        }\n",
    "        self.out_ext_channels = self.features_size[backbone][0]\n",
    "        \n",
    "        # self.flatten_type = flatten_type # in ['patch', 'channel']\n",
    "        # self.version = version  # in ['ca-rgb_cat-0.5', 'ca-freq_cat-0.5']\n",
    "        # self.position_embed = position_embed\n",
    "        # self.pool = pool\n",
    "        # self.conv_attn = conv_attn\n",
    "        self.activation = self.get_activation(act)\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.rgb_extractor = self.get_feature_extractor(architecture=backbone, pretrained=pretrained, unfreeze_blocks=unfreeze_blocks, num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(architecture=backbone, pretrained=pretrained, unfreeze_blocks=unfreeze_blocks, num_classes=num_classes, in_channels=1)     \n",
    "        self.normalize_ifft = normalize_ifft\n",
    "        if self.normalize_ifft == 'batchnorm':\n",
    "            self.batchnorm_ifft = nn.BatchNorm2d(num_features=self.out_ext_channels if classifier == 'mlp' else 320)\n",
    "        if self.normalize_ifft == 'layernorm':\n",
    "            self.layernorm_ifft = nn.LayerNorm(normalized_shape=self.features_size[self.backbone])\n",
    "        ############################# PATCH CONFIG ################################\n",
    "\n",
    "        # self.CA = CrossAttention(in_dim=self.in_dim, inner_dim=inner_ca_dim, prj_out=prj_out, qkv_embed=qkv_embed, init_weight=init_ca_weight)\n",
    "        device = torch.device('cpu')\n",
    "        self.cma = CrossModalAttention(in_dim=self.out_ext_channels if classifier=='mlp' else 320, activation=self.activation, ratio=4, cross_value=True, gamma_cma=gamma_cma).to(device)\n",
    "\n",
    "        # Thêm 1 embedding vector cho classify token:\n",
    "        # self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "        # self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer_block_4 = nn.ModuleList([])\n",
    "        for _ in range(depth_block4):\n",
    "            self.transformer_block_4.append(PatchTrans(in_channel=40, in_size=16, patch_resolution='1-2-4-8').to(device))\n",
    "        self.transformer_block_10_rgb = PatchTransv2(in_channel=112, in_size=8, patch_resolution='1-2-4-8').to(device)\n",
    "        self.transformer_block_10_freq = PatchTransv2(in_channel=112, in_size=8, patch_resolution='1-2-4-8').to(device)\n",
    "\n",
    "        # Classifier:\n",
    "        self.classifier = classifier\n",
    "        if self.classifier == 'mlp':\n",
    "            self.mlp_dropout = nn.Dropout(dropout_in_mlp)\n",
    "            self.mlp_hidden = nn.Linear(self.dim, self.mlp_dim)\n",
    "            self.mlp_relu = nn.ReLU()\n",
    "            self.mlp_out = nn.Linear(self.mlp_dim, self.num_classes)\n",
    "\n",
    "        if self.classifier == 'vit':\n",
    "            self.convr = nn.Conv2d(in_channels=320, out_channels=in_vit_channels, kernel_size=1)\n",
    "            self.embedding = nn.Linear(self.patch_size*self.patch_size *in_vit_channels if flatten_type=='patch' else 16, self.dim)\n",
    "            self.dropout = nn.Dropout(self.emb_dropout)\n",
    "            self.transformer = Transformer(self.dim, self.depth_vit, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "            self.mlp_dropout = nn.Dropout(dropout_in_mlp)\n",
    "            self.mlp_hidden = nn.Linear(self.dim, self.mlp_dim)\n",
    "            self.mlp_relu = nn.ReLU()\n",
    "            self.mlp_out = nn.Linear(self.mlp_dim, self.num_classes)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.init_weights(init_type=init_type)\n",
    "\n",
    "    def get_activation(self, act):\n",
    "        if act == 'relu':\n",
    "            activation = nn.ReLU(inplace=True)\n",
    "        elif act == 'leakyrelu':\n",
    "            activation = nn.LeakyReLU(0.01, inplace=True)\n",
    "        elif act == 'tanh':\n",
    "            activation = nn.Tanh()\n",
    "        elif act == 'sigmoid':\n",
    "            activation = nn.Sigmoid()\n",
    "        elif act == 'selu':\n",
    "            activation = nn.SELU()\n",
    "        else:\n",
    "            activation = None\n",
    "        return activation\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", unfreeze_blocks=-1, pretrained=False, num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels, pretrained=bool(pretrained))\n",
    "            # extractor._blocks[11]._depthwise_conv = Conv2dStaticSamePadding(in_channels=672, out_channels=672, kernel_size=(5, 5), stride=(1, 1), groups=672, image_size=224)\n",
    "            # extractor._conv_head = nn.Identity()\n",
    "            if unfreeze_blocks != -1:\n",
    "                # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "                for i in range(0, len(extractor._blocks)):\n",
    "                    for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "                        if i >= len(extractor._blocks) - unfreeze_blocks:\n",
    "                            param.requires_grad = True\n",
    "                        else:\n",
    "                            param.requires_grad = False\n",
    "            # print(extractor)\n",
    "        \n",
    "        if architecture == 'xception_net':\n",
    "            xception = get_model(\"xception\", pretrained=bool(pretrained))\n",
    "            extractor = nn.Sequential(*list(xception.children())[:-1])\n",
    "            extractor[0].final_block.pool = nn.Identity()\n",
    "            if in_channels != 3:\n",
    "                extractor[0].init_block.conv1.conv = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "\n",
    "            if unfreeze_blocks != -1:\n",
    "                blocks = len(extractor[0].children())\n",
    "                print(\"Number of blocks in xception: \", len(blocks))\n",
    "                for i, block in enumerate(extractor[0].children()):\n",
    "                    if i >= blocks - unfreeze_blocks:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    else:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = False\n",
    "        print(\"Pretrained backbone: \", bool(pretrained))\n",
    "        # if not pretrained:\n",
    "        #     self.init_conv_weight(extractor)\n",
    "        return extractor\n",
    "\n",
    "    def init_weights(self, init_type='normal', gain=0.02):\n",
    "        '''\n",
    "        initialize network's weights\n",
    "        init_type: normal | xavier | kaiming | orthogonal\n",
    "        https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9451e70673400885567d08a9e97ade2524c700d0/models/networks.py#L39\n",
    "        '''\n",
    "\n",
    "        def init_func(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if classname.find('InstanceNorm2d') != -1:\n",
    "                if hasattr(m, 'weight') and m.weight is not None:\n",
    "                    nn.init.constant_(m.weight.data, 1.0)\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "            elif hasattr(m, 'weight') and (\n",
    "                classname.find('Conv') != -1 or classname.find('Linear') != -1\n",
    "            ):\n",
    "                if init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight.data, 0.0, gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "                elif init_type == 'xavier_uniform':\n",
    "                    nn.init.xavier_uniform_(m.weight.data, gain=1.0)\n",
    "                elif init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    nn.init.orthogonal_(m.weight.data, gain=gain)\n",
    "                elif init_type == 'none':  # uses pytorch's default init method\n",
    "                    m.reset_parameters()\n",
    "                else:\n",
    "                    raise NotImplementedError(\n",
    "                        'initialization method [%s] is not implemented'\n",
    "                        % init_type\n",
    "                    )\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        self.apply(init_func)\n",
    "\n",
    "        for m in self.children():\n",
    "            if hasattr(m, 'init_weights'):\n",
    "                m.init_weights(init_type, gain)\n",
    "\n",
    "    def ifft(self, freq_feature, norm_type='none'):\n",
    "        ifreq_feature = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_feature))) + 1e-10)  # Hơi ảo???\n",
    "        if norm_type == 'none':\n",
    "            pass\n",
    "        elif norm_type == 'batchnorm':\n",
    "            ifreq_feature = self.batchnorm_ifft(ifreq_feature)\n",
    "        elif norm_type == 'layernorm':\n",
    "            ifreq_feature = self.layernorm_ifft(ifreq_feature)\n",
    "        elif norm_type == 'normal':\n",
    "            ifreq_feature = F.normalize(ifreq_feature)\n",
    "        return ifreq_feature\n",
    "\n",
    "    def flatten_to_vectors(self, feature):\n",
    "        vectors = None\n",
    "        if self.flatten_type == 'patch':\n",
    "            vectors = rearrange(feature, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "        elif self.flatten_type == 'channel':\n",
    "            vectors = rearrange(feature, 'b c h w -> b c (h w)')\n",
    "        else:\n",
    "            pass\n",
    "        return vectors\n",
    "\n",
    "    def extract_feature(self, rgb_imgs, freq_imgs):\n",
    "        if self.backbone == 'efficient_net':\n",
    "            #\n",
    "            rgb_features = self.rgb_extractor.extract_features_block_4(rgb_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "            for attn in self.transformer_block_4:\n",
    "                rgb_features = attn(rgb_features)\n",
    "            freq_features = self.freq_extractor.extract_features_block_4(freq_imgs)              # shape (batchsize, 1280, 4, 4)\n",
    "            #\n",
    "            rgb_features = self.rgb_extractor.extract_features_block_11(rgb_features)\n",
    "            freq_features = self.freq_extractor.extract_features_block_11(freq_features)\n",
    "            rgb_features_1 = self.transformer_block_10_rgb(rgb_features, freq_features)\n",
    "            freq_features_1 = self.transformer_block_10_freq(freq_features, rgb_features)\n",
    "            rgb_features = self.rgb_extractor.extract_features_last_block(rgb_features_1, classifier=self.classifier)\n",
    "            freq_features = self.freq_extractor.extract_features_last_block(freq_features_1, classifier=self.classifier)\n",
    "        else:\n",
    "            rgb_features = self.rgb_extractor(rgb_imgs)\n",
    "            freq_features = self.freq_extractor(freq_imgs)\n",
    "        return rgb_features, freq_features\n",
    "\n",
    "    def forward_once(self, rgb_imgs, freq_imgs):\n",
    "        rgb_features, freq_features = self.extract_feature(rgb_imgs, freq_imgs)\n",
    "        ifreq_features = self.ifft(freq_features, norm_type=self.normalize_ifft)\n",
    "        # print(\"Features shape: \", rgb_features.shape, freq_features.shape, ifreq_features.shape)\n",
    "        out = self.cma(rgb_features, ifreq_features, freq_features)\n",
    "\n",
    "        if self.classifier == 'mlp':\n",
    "            x = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "            x = x.squeeze().squeeze()\n",
    "            x = self.mlp_dropout(x)\n",
    "            y = self.mlp_hidden(x)\n",
    "            x = self.mlp_relu(y)\n",
    "            x = self.mlp_dropout(x)\n",
    "            x = self.mlp_out(x)\n",
    "\n",
    "        if self.classifier == 'vit':\n",
    "            x = self.convr(out)\n",
    "            # print(x.shape)\n",
    "            x = self.flatten_to_vectors(x)\n",
    "            x = self.embedding(x)\n",
    "            # print(x.shape)\n",
    "            x = self.dropout(x)\n",
    "            x = self.transformer(x)\n",
    "            x = x.mean(dim = 1)\n",
    "            x = self.mlp_dropout(x)\n",
    "            y = self.mlp_hidden(x)\n",
    "            x = self.mlp_relu(y)\n",
    "            x = self.mlp_dropout(x)\n",
    "            x = self.mlp_out(x)\n",
    "        return y, self.sigmoid(x)\n",
    "\n",
    "    def forward(self, rgb_imgs0, freq_imgs0, rgb_imgs1, freq_imgs1):\n",
    "        embedding0, out0 = self.forward_once(rgb_imgs0, freq_imgs0)\n",
    "        embedding1, out1 = self.forward_once(rgb_imgs1, freq_imgs1)\n",
    "        return embedding0, out0, embedding1, out1\n",
    "\n",
    "from torchsummary import summary\n",
    "if __name__ == '__main__':\n",
    "    x = torch.ones(32, 3, 128, 128)\n",
    "    y = torch.ones(32, 1, 128, 128)\n",
    "    model_ = PairwiseDualPatchCNNCMAViT(image_size=128, num_classes=1, depth_block4=2, \\\n",
    "                backbone='efficient_net', pretrained=True, unfreeze_blocks=-1, \\\n",
    "                normalize_ifft='batchnorm',\\\n",
    "                act='selu',\\\n",
    "                init_type=\"xavier_uniform\", \\\n",
    "                gamma_cma=-1, flatten_type='patch', patch_size=2, \\\n",
    "                dim=1024, depth_vit=2, heads=3, dim_head=64, dropout=0.15, emb_dropout=0.15, mlp_dim=2048, dropout_in_mlp=0.0, \\\n",
    "                classifier='vit', in_vit_channels=256)\n",
    "    out0, out1, out2, out3 = model_(x, y, x, y)\n",
    "    # print(out.shape)\n",
    "    summary(model_, [(3, 128, 128), (1, 128, 128), (3, 128, 128), (1, 128, 128)], batch_size=24, device='cpu')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73e4523d5c5fcabc881bfbabdc03d28b885253c65d62f8f3eb31939c7679911f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('phucnp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
