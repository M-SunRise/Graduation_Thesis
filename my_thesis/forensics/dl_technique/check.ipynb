{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained backbone:  True\n",
      "Pretrained backbone:  True\n",
      "Model 2\n",
      "1:  tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.5207, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\n",
      "2:  tensor([[[-23.0259, -23.0259,  -0.9929,  ..., -23.0259, -23.0259,  -1.0684],\n",
      "         [-23.0259, -23.0259,  -1.4588,  ..., -23.0259, -23.0259,  -1.7812],\n",
      "         [-23.0259, -23.0259,  -2.7205,  ..., -23.0259, -23.0259,  -1.7615],\n",
      "         ...,\n",
      "         [-23.0259, -23.0259,  -2.0983,  ..., -23.0259, -23.0259,  -2.0188],\n",
      "         [-23.0259, -23.0259,  -3.4427,  ..., -23.0259, -23.0259,  -3.1820],\n",
      "         [-23.0259, -23.0259,  -2.6671,  ..., -23.0259, -23.0259,  -2.4188]]])\n",
      "3:  tensor([[[-23.0259, -23.0259,  -0.9929,  ..., -23.0259, -23.0259,  -1.0684],\n",
      "         [-23.0259, -23.0259,  -1.4588,  ..., -23.0259, -23.0259,  -1.7812],\n",
      "         [-23.0259, -23.0259,  -2.7205,  ..., -23.0259, -23.0259,  -1.7615],\n",
      "         ...,\n",
      "         [-23.0259, -23.0259,  -2.0983,  ..., -23.0259, -23.0259,  -2.0188],\n",
      "         [-23.0259, -23.0259,  -3.4427,  ..., -23.0259, -23.0259,  -3.1820],\n",
      "         [-23.0259, -23.0259,  -2.6671,  ..., -23.0259, -23.0259,  -2.4188]]])\n",
      "q before scaledot:  tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.5207, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\n",
      "k before scaledot:  tensor([[[-23.0259, -23.0259,  -0.9929,  ..., -23.0259, -23.0259,  -1.0684],\n",
      "         [-23.0259, -23.0259,  -1.4588,  ..., -23.0259, -23.0259,  -1.7812],\n",
      "         [-23.0259, -23.0259,  -2.7205,  ..., -23.0259, -23.0259,  -1.7615],\n",
      "         ...,\n",
      "         [-23.0259, -23.0259,  -2.0983,  ..., -23.0259, -23.0259,  -2.0188],\n",
      "         [-23.0259, -23.0259,  -3.4427,  ..., -23.0259, -23.0259,  -3.1820],\n",
      "         [-23.0259, -23.0259,  -2.6671,  ..., -23.0259, -23.0259,  -2.4188]]])\n",
      "v before scaledot:  tensor([[[-23.0259, -23.0259,  -0.9929,  ..., -23.0259, -23.0259,  -1.0684],\n",
      "         [-23.0259, -23.0259,  -1.4588,  ..., -23.0259, -23.0259,  -1.7812],\n",
      "         [-23.0259, -23.0259,  -2.7205,  ..., -23.0259, -23.0259,  -1.7615],\n",
      "         ...,\n",
      "         [-23.0259, -23.0259,  -2.0983,  ..., -23.0259, -23.0259,  -2.0188],\n",
      "         [-23.0259, -23.0259,  -3.4427,  ..., -23.0259, -23.0259,  -3.1820],\n",
      "         [-23.0259, -23.0259,  -2.6671,  ..., -23.0259, -23.0259,  -2.4188]]])\n",
      "in scale dot.\n",
      "q after:  tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0336, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\n",
      "attn:  tensor([[[9.2665e-02, 7.3087e-02, 6.2955e-02, 7.3087e-02, 7.2115e-02,\n",
      "          6.0219e-02, 5.2029e-02, 6.1115e-02, 6.0567e-02, 5.1627e-02,\n",
      "          4.3428e-02, 5.1627e-02, 7.2115e-02, 6.1115e-02, 5.2029e-02,\n",
      "          6.0219e-02],\n",
      "         [1.2678e-01, 8.2234e-02, 6.4058e-02, 8.2234e-02, 8.7158e-02,\n",
      "          5.7335e-02, 4.7171e-02, 5.5884e-02, 4.9613e-02, 3.7843e-02,\n",
      "          2.4298e-02, 3.7843e-02, 8.7158e-02, 5.5884e-02, 4.7171e-02,\n",
      "          5.7335e-02],\n",
      "         [4.9491e-01, 9.5646e-02, 2.9022e-02, 9.5646e-02, 9.2205e-02,\n",
      "          1.6976e-02, 5.8534e-03, 1.9223e-02, 9.8554e-03, 2.9245e-03,\n",
      "          5.5868e-04, 2.9245e-03, 9.2205e-02, 1.9223e-02, 5.8534e-03,\n",
      "          1.6976e-02],\n",
      "         [1.5655e-01, 9.5632e-02, 6.1911e-02, 9.5632e-02, 9.2519e-02,\n",
      "          5.5516e-02, 3.9672e-02, 5.3134e-02, 3.7824e-02, 2.8524e-02,\n",
      "          1.3718e-02, 2.8524e-02, 9.2519e-02, 5.3134e-02, 3.9672e-02,\n",
      "          5.5516e-02],\n",
      "         [1.6794e-01, 8.9780e-02, 5.6694e-02, 8.9780e-02, 9.7313e-02,\n",
      "          5.5640e-02, 3.2409e-02, 5.7460e-02, 4.5854e-02, 2.4567e-02,\n",
      "          1.5170e-02, 2.4567e-02, 9.7313e-02, 5.7460e-02, 3.2409e-02,\n",
      "          5.5640e-02],\n",
      "         [2.5059e-01, 9.4649e-02, 6.4713e-02, 9.4649e-02, 1.0315e-01,\n",
      "          4.1026e-02, 2.8647e-02, 4.2491e-02, 3.0647e-02, 1.3936e-02,\n",
      "          6.2401e-03, 1.3936e-02, 1.0315e-01, 4.2491e-02, 2.8647e-02,\n",
      "          4.1026e-02],\n",
      "         [6.3413e-01, 7.4785e-02, 1.3445e-02, 7.4785e-02, 7.8826e-02,\n",
      "          8.3641e-03, 1.7839e-03, 1.0583e-02, 2.6740e-03, 5.0458e-04,\n",
      "          5.8218e-05, 5.0458e-04, 7.8826e-02, 1.0583e-02, 1.7839e-03,\n",
      "          8.3641e-03],\n",
      "         [1.4353e-01, 9.0474e-02, 6.8533e-02, 9.0474e-02, 9.0366e-02,\n",
      "          5.8048e-02, 4.4449e-02, 5.3508e-02, 3.9683e-02, 2.9923e-02,\n",
      "          1.4712e-02, 2.9923e-02, 9.0366e-02, 5.3508e-02, 4.4449e-02,\n",
      "          5.8048e-02],\n",
      "         [8.1765e-02, 7.0843e-02, 6.4070e-02, 7.0843e-02, 7.1859e-02,\n",
      "          6.1280e-02, 5.5749e-02, 6.3202e-02, 6.0217e-02, 5.2425e-02,\n",
      "          4.3233e-02, 5.2425e-02, 7.1859e-02, 6.3202e-02, 5.5749e-02,\n",
      "          6.1280e-02],\n",
      "         [1.4361e-01, 8.6017e-02, 6.9260e-02, 8.6017e-02, 9.2072e-02,\n",
      "          5.5618e-02, 4.3028e-02, 5.6275e-02, 4.2386e-02, 3.0563e-02,\n",
      "          1.7592e-02, 3.0563e-02, 9.2072e-02, 5.6275e-02, 4.3028e-02,\n",
      "          5.5618e-02],\n",
      "         [9.4051e-02, 7.3532e-02, 6.4102e-02, 7.3532e-02, 8.0175e-02,\n",
      "          6.1815e-02, 5.3944e-02, 6.1833e-02, 5.6359e-02, 4.3643e-02,\n",
      "          3.5605e-02, 4.3643e-02, 8.0175e-02, 6.1833e-02, 5.3944e-02,\n",
      "          6.1815e-02],\n",
      "         [7.0707e-02, 6.6271e-02, 6.5342e-02, 6.6271e-02, 6.7694e-02,\n",
      "          6.3455e-02, 6.2629e-02, 6.2844e-02, 5.8511e-02, 5.4870e-02,\n",
      "          4.9916e-02, 5.4870e-02, 6.7694e-02, 6.2844e-02, 6.2629e-02,\n",
      "          6.3455e-02],\n",
      "         [6.8587e-02, 6.5700e-02, 6.3197e-02, 6.5700e-02, 6.5322e-02,\n",
      "          6.2167e-02, 5.9682e-02, 6.3107e-02, 6.2057e-02, 5.9111e-02,\n",
      "          5.5981e-02, 5.9111e-02, 6.5322e-02, 6.3107e-02, 5.9682e-02,\n",
      "          6.2167e-02],\n",
      "         [1.0002e-01, 7.4935e-02, 6.4421e-02, 7.4935e-02, 7.8632e-02,\n",
      "          6.1268e-02, 5.1629e-02, 6.1453e-02, 5.8182e-02, 4.4797e-02,\n",
      "          3.1952e-02, 4.4797e-02, 7.8632e-02, 6.1453e-02, 5.1629e-02,\n",
      "          6.1268e-02],\n",
      "         [7.7404e-02, 6.7466e-02, 6.4129e-02, 6.7466e-02, 7.0941e-02,\n",
      "          6.2436e-02, 5.8451e-02, 6.1611e-02, 6.0841e-02, 5.3717e-02,\n",
      "          4.8379e-02, 5.3717e-02, 7.0941e-02, 6.1611e-02, 5.8451e-02,\n",
      "          6.2436e-02],\n",
      "         [6.7638e-02, 6.4589e-02, 6.3474e-02, 6.4589e-02, 6.5514e-02,\n",
      "          6.3358e-02, 6.1937e-02, 6.2130e-02, 6.1393e-02, 5.9295e-02,\n",
      "          5.3845e-02, 5.9295e-02, 6.5514e-02, 6.2130e-02, 6.1937e-02,\n",
      "          6.3358e-02]]])\n",
      "Attn weights: \n",
      " tensor([[[-23.0259, -23.0259,  -2.4759,  ..., -23.0259, -23.0259,  -2.3594],\n",
      "         [-23.0259, -23.0259,  -2.3020,  ..., -23.0259, -23.0259,  -2.2352],\n",
      "         [-23.0258, -23.0258,  -1.4556,  ..., -23.0258, -23.0258,  -1.6012],\n",
      "         ...,\n",
      "         [-23.0258, -23.0258,  -2.4193,  ..., -23.0258, -23.0258,  -2.3107],\n",
      "         [-23.0258, -23.0258,  -2.5356,  ..., -23.0258, -23.0258,  -2.4091],\n",
      "         [-23.0258, -23.0258,  -2.5864,  ..., -23.0258, -23.0258,  -2.4521]]]) tensor([[[9.2665e-02, 7.3087e-02, 6.2955e-02, 7.3087e-02, 7.2115e-02,\n",
      "          6.0219e-02, 5.2029e-02, 6.1115e-02, 6.0567e-02, 5.1627e-02,\n",
      "          4.3428e-02, 5.1627e-02, 7.2115e-02, 6.1115e-02, 5.2029e-02,\n",
      "          6.0219e-02],\n",
      "         [1.2678e-01, 8.2234e-02, 6.4058e-02, 8.2234e-02, 8.7158e-02,\n",
      "          5.7335e-02, 4.7171e-02, 5.5884e-02, 4.9613e-02, 3.7843e-02,\n",
      "          2.4298e-02, 3.7843e-02, 8.7158e-02, 5.5884e-02, 4.7171e-02,\n",
      "          5.7335e-02],\n",
      "         [4.9491e-01, 9.5646e-02, 2.9022e-02, 9.5646e-02, 9.2205e-02,\n",
      "          1.6976e-02, 5.8534e-03, 1.9223e-02, 9.8554e-03, 2.9245e-03,\n",
      "          5.5868e-04, 2.9245e-03, 9.2205e-02, 1.9223e-02, 5.8534e-03,\n",
      "          1.6976e-02],\n",
      "         [1.5655e-01, 9.5632e-02, 6.1911e-02, 9.5632e-02, 9.2519e-02,\n",
      "          5.5516e-02, 3.9672e-02, 5.3134e-02, 3.7824e-02, 2.8524e-02,\n",
      "          1.3718e-02, 2.8524e-02, 9.2519e-02, 5.3134e-02, 3.9672e-02,\n",
      "          5.5516e-02],\n",
      "         [1.6794e-01, 8.9780e-02, 5.6694e-02, 8.9780e-02, 9.7313e-02,\n",
      "          5.5640e-02, 3.2409e-02, 5.7460e-02, 4.5854e-02, 2.4567e-02,\n",
      "          1.5170e-02, 2.4567e-02, 9.7313e-02, 5.7460e-02, 3.2409e-02,\n",
      "          5.5640e-02],\n",
      "         [2.5059e-01, 9.4649e-02, 6.4713e-02, 9.4649e-02, 1.0315e-01,\n",
      "          4.1026e-02, 2.8647e-02, 4.2491e-02, 3.0647e-02, 1.3936e-02,\n",
      "          6.2401e-03, 1.3936e-02, 1.0315e-01, 4.2491e-02, 2.8647e-02,\n",
      "          4.1026e-02],\n",
      "         [6.3413e-01, 7.4785e-02, 1.3445e-02, 7.4785e-02, 7.8826e-02,\n",
      "          8.3641e-03, 1.7839e-03, 1.0583e-02, 2.6740e-03, 5.0458e-04,\n",
      "          5.8218e-05, 5.0458e-04, 7.8826e-02, 1.0583e-02, 1.7839e-03,\n",
      "          8.3641e-03],\n",
      "         [1.4353e-01, 9.0474e-02, 6.8533e-02, 9.0474e-02, 9.0366e-02,\n",
      "          5.8048e-02, 4.4449e-02, 5.3508e-02, 3.9683e-02, 2.9923e-02,\n",
      "          1.4712e-02, 2.9923e-02, 9.0366e-02, 5.3508e-02, 4.4449e-02,\n",
      "          5.8048e-02],\n",
      "         [8.1765e-02, 7.0843e-02, 6.4070e-02, 7.0843e-02, 7.1859e-02,\n",
      "          6.1280e-02, 5.5749e-02, 6.3202e-02, 6.0217e-02, 5.2425e-02,\n",
      "          4.3233e-02, 5.2425e-02, 7.1859e-02, 6.3202e-02, 5.5749e-02,\n",
      "          6.1280e-02],\n",
      "         [1.4361e-01, 8.6017e-02, 6.9260e-02, 8.6017e-02, 9.2072e-02,\n",
      "          5.5618e-02, 4.3028e-02, 5.6275e-02, 4.2386e-02, 3.0563e-02,\n",
      "          1.7592e-02, 3.0563e-02, 9.2072e-02, 5.6275e-02, 4.3028e-02,\n",
      "          5.5618e-02],\n",
      "         [9.4051e-02, 7.3532e-02, 6.4102e-02, 7.3532e-02, 8.0175e-02,\n",
      "          6.1815e-02, 5.3944e-02, 6.1833e-02, 5.6359e-02, 4.3643e-02,\n",
      "          3.5605e-02, 4.3643e-02, 8.0175e-02, 6.1833e-02, 5.3944e-02,\n",
      "          6.1815e-02],\n",
      "         [7.0707e-02, 6.6271e-02, 6.5342e-02, 6.6271e-02, 6.7694e-02,\n",
      "          6.3455e-02, 6.2629e-02, 6.2844e-02, 5.8511e-02, 5.4870e-02,\n",
      "          4.9916e-02, 5.4870e-02, 6.7694e-02, 6.2844e-02, 6.2629e-02,\n",
      "          6.3455e-02],\n",
      "         [6.8587e-02, 6.5700e-02, 6.3197e-02, 6.5700e-02, 6.5322e-02,\n",
      "          6.2167e-02, 5.9682e-02, 6.3107e-02, 6.2057e-02, 5.9111e-02,\n",
      "          5.5981e-02, 5.9111e-02, 6.5322e-02, 6.3107e-02, 5.9682e-02,\n",
      "          6.2167e-02],\n",
      "         [1.0002e-01, 7.4935e-02, 6.4421e-02, 7.4935e-02, 7.8632e-02,\n",
      "          6.1268e-02, 5.1629e-02, 6.1453e-02, 5.8182e-02, 4.4797e-02,\n",
      "          3.1952e-02, 4.4797e-02, 7.8632e-02, 6.1453e-02, 5.1629e-02,\n",
      "          6.1268e-02],\n",
      "         [7.7404e-02, 6.7466e-02, 6.4129e-02, 6.7466e-02, 7.0941e-02,\n",
      "          6.2436e-02, 5.8451e-02, 6.1611e-02, 6.0841e-02, 5.3717e-02,\n",
      "          4.8379e-02, 5.3717e-02, 7.0941e-02, 6.1611e-02, 5.8451e-02,\n",
      "          6.2436e-02],\n",
      "         [6.7638e-02, 6.4589e-02, 6.3474e-02, 6.4589e-02, 6.5514e-02,\n",
      "          6.3358e-02, 6.1937e-02, 6.2130e-02, 6.1393e-02, 5.9295e-02,\n",
      "          5.3845e-02, 5.9295e-02, 6.5514e-02, 6.2130e-02, 6.1937e-02,\n",
      "          6.3358e-02]]])\n",
      "Fusion out: \n",
      " tensor([[[0.0000, 0.0995, 0.0381,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0805, 0.0278,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0418, 0.0029,  ..., 1.5207, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0881, 0.0332,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.1026, 0.0397,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.1085, 0.0430,  ..., 0.0000, 0.0000, 0.0000]]])\n",
      "Embed:  tensor([[[ 0.4400, -0.2908,  0.1130,  ...,  0.4323, -0.3570,  0.6698],\n",
      "         [ 0.4635, -0.2666,  0.1990,  ...,  0.7691, -0.3332,  0.6753],\n",
      "         [ 0.8860,  0.0323, -0.2752,  ...,  0.7945, -0.2642,  1.0639],\n",
      "         ...,\n",
      "         [ 0.4712, -0.1816,  0.4048,  ...,  0.4538, -0.2402,  0.8847],\n",
      "         [ 0.4837, -0.1844,  0.3661,  ...,  0.6105, -0.4101,  0.6929],\n",
      "         [ 0.4586, -0.1195,  0.2691,  ...,  0.4733, -0.3475,  0.7071]]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/check.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 550>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/check.ipynb#ch0000000vscode-remote?line=576'>577</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/check.ipynb#ch0000000vscode-remote?line=577'>578</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel 2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/check.ipynb#ch0000000vscode-remote?line=578'>579</a>\u001b[0m     rgb_2, freq_2, ifreq_2 \u001b[39m=\u001b[39m model_2(x, y)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/check.ipynb#ch0000000vscode-remote?line=579'>580</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m************************************* RGB 2 ***\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.191.57.61/mnt/disk1/doan/phucnp/Graduation_Thesis/my_thesis/forensics/dl_technique/check.ipynb#ch0000000vscode-remote?line=580'>581</a>\u001b[0m     \u001b[39mprint\u001b[39m(freq_2)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit import Transformer\n",
    "from pytorchcv.model_provider import get_model\n",
    "\n",
    "class OriDualEfficientViT(nn.Module):\n",
    "    def __init__(self, channels=1280,\\\n",
    "                 image_size=224,patch_size=7,num_classes=1,dim=1024,\\\n",
    "                 depth=6,heads=8,mlp_dim=2048,\\\n",
    "                 emb_dim=32, dim_head=64,dropout=0.15,emb_dropout=0.15,backbone=\"xception_net\", version=\"cross_attention-spatial-cat\",weight=0.5,freeze=0):  \n",
    "        super(OriDualEfficientViT, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.features_size = {\n",
    "            128: (4, 4),\n",
    "            224: (7, 7),\n",
    "            256: (8, 8)\n",
    "        }\n",
    "        \n",
    "        # \"cross_attention-spatial-cat\": sử dụng cross-attention, cat với spatial vectors output\n",
    "        # \"cross_attention-spatial-add\": sử dụng cross-attention, add với spatial vectors output\n",
    "        # \"cross_attention-freq-cat\": sử dụng cross-attention, cat với freq vectors\n",
    "        # \"cross_attention-freq-add\": sử dụng cross-attention, add với freq vectors\n",
    "        # \"merge-add\": cộng thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        # \"merge-cat\": cat thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        self.version = version\n",
    "        self.weight = weight\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.spatial_extractor = self.get_feature_extractor(freeze=freeze, architecture=backbone, num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(freeze=freeze, architecture=backbone, num_classes=num_classes, in_channels=1)\n",
    "\n",
    "        ############################# Xét 2 stream hiện tại là như nhau\n",
    "        # Kích thước của 1 patch\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "        # Số lượng patches\n",
    "        self.num_patches = int((self.features_size[image_size][0] * self.features_size[image_size][1]) / (self.patch_size * self.patch_size))\n",
    "        # Patch_dim = P^2 * C\n",
    "        self.patch_dim = channels * (self.patch_size ** 2)\n",
    "\n",
    "        # print(\"Num patches: \", self.num_patches)\n",
    "        # print(\"Patch dim: \", self.patch_dim)\n",
    "\n",
    "        # Embed vị trí cho từng patch\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches+1, self.dim))\n",
    "        # self.pos_embedding_1 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_2 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_3 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "\n",
    "        # Đưa flatten vector của feature maps về chiều cố định của vector trong transformer.\n",
    "        # self.patch_to_embedding_1 = nn.Linear(self.patch_dim, self.dim)\n",
    "        # self.patch_to_embedding_2 = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Giảm chiều vector sau concat 2*patch_dim về D:\n",
    "        self.patch_to_embedding_cat = nn.Linear(2*self.patch_dim, self.dim)\n",
    "        self.patch_to_embedding_add = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Thêm 1 embedding vector cho classify token:\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"xception_net\", freeze=0, pretrained=True, num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            if pretrained == \"\":\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b0', pretrained=True, num_classes=num_classes,in_channels = in_channels)\n",
    "            else:\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b7', num_classes=num_classes,in_channels = in_channels)\n",
    "                # Load checkpoint\n",
    "                checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "                state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
    "                # Load weights\n",
    "                extractor.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=False)\n",
    "\n",
    "            if freeze:\n",
    "            # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "                print(\"Here\")\n",
    "                for i in range(0, len(extractor._blocks)):\n",
    "                    for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "                        if i >= len(extractor._blocks) - 3:\n",
    "                            param.requires_grad = True\n",
    "                        else:\n",
    "                            param.requires_grad = False\n",
    "        if architecture == 'xception_net':\n",
    "            xception = get_model(\"xception\", pretrained=bool(pretrained))\n",
    "            extractor = nn.Sequential(*list(xception.children())[:-1])\n",
    "            extractor[0].final_block.pool = nn.Identity()\n",
    "            if in_channels != 3:\n",
    "                extractor[0].init_block.conv1.conv = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "\n",
    "        print(\"Pretrained backbone: \", bool(pretrained))\n",
    "        return extractor\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        print(\"in scale dot.\")\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        print('q after: ', q)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        print(\"attn: \", attn)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "    def cross_attention(self, spatials, ifreqs):\n",
    "        \"\"\"\n",
    "            spatials: (B, N, D) --> Query,\n",
    "            freqs: (B, N, D) --> Key\n",
    "            output: \n",
    "        \"\"\"\n",
    "        print(\"q before scaledot: \", spatials)\n",
    "        print(\"k before scaledot: \", ifreqs)\n",
    "        print(\"v before scaledot: \", ifreqs)\n",
    "        emb_dim = spatials.shape[2]\n",
    "        assert emb_dim == ifreqs.shape[2]\n",
    "        attn_outputs, attn_weights = self.scale_dot(spatials, ifreqs, ifreqs)\n",
    "        return attn_outputs, attn_weights\n",
    "\n",
    "    def extract_feature(self, rgb_imgs, freq_imgs):\n",
    "        if self.backbone == 'efficient_net':\n",
    "            rgb_features = self.spatial_extractor.extract_features(rgb_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "            freq_features = self.freq_extractor.extract_features(freq_imgs)              # shape (batchsize, 1280, 4, 4)\n",
    "        else:\n",
    "            rgb_features = self.spatial_extractor(rgb_imgs)\n",
    "            freq_features = self.freq_extractor(freq_imgs)\n",
    "        return rgb_features, freq_features\n",
    "\n",
    "    def forward(self, spatial_imgs, frequency_imgs):\n",
    "        p = self.patch_size\n",
    "        # Extract features\n",
    "        # print(frequency_imgs)\n",
    "        spatial_features, freq_features = self.extract_feature(spatial_imgs, frequency_imgs)                     # shape (batchsize, 1280, 8, 8)conda\n",
    "        ifreq_features = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_features))) + 1e-10)  # Hơi ảo???\n",
    "        # return spatial_features, freq_features, ifreq_features\n",
    "        # print(ifreq_features.shape)\n",
    "        # assert(ifreq_features.shape == freq_features.shape)\n",
    "        # print(\"Features shape: \", spatial_features.shape, freq_features.shape)\n",
    "\n",
    "        # Flatten to vector:\n",
    "        spatial_vectors = rearrange(spatial_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        freq_vectors = rearrange(freq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        ifreq_vectors = rearrange(ifreq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "\n",
    "        assert self.patch_dim == spatial_vectors.shape[2]\n",
    "        assert self.patch_dim == freq_vectors.shape[2]\n",
    "\n",
    "        embed = None\n",
    "        print(\"1: \", spatial_vectors)\n",
    "        print(\"2: \", ifreq_vectors)\n",
    "        print(\"3: \", ifreq_vectors)\n",
    "        if \"cross_attention\" in self.version:          # Merge using cross-attention  \n",
    "            ########## Patch embedding and add position embedding to each domain:\n",
    "            # spatial_vectors = self.patch_to_embedding_1(spatial_vectors)\n",
    "            # spatial_vectors += self.pos_embedding_1\n",
    "\n",
    "            # freq_vectors = self.patch_to_embedding_2(freq_vectors)\n",
    "            # freq_vectors += self.pos_embedding_2\n",
    "\n",
    "            # ifreq_vectors = self.patch_to_embedding_2(ifreq_vectors)\n",
    "            # ifreq_vectors += self.pos_embedding_2  \n",
    "            # print(\"Step 2 shape: \", spatial_vectors.shape, freq_vectors.shape)  # (batchsize, num_patches, D)\n",
    "            ##########\n",
    "        \n",
    "            # Cal attn weight between ifreq and spatial vectors:\n",
    "            # Cross-attention (spatial-decoder, ifreq-encoder)\n",
    "            attn_outputs, attn_weights = self.cross_attention(spatial_vectors, ifreq_vectors)     # Shape: (), (batchsize, num_patches, num_patches)\n",
    "            print(\"Attn weights: \\n\", attn_outputs, attn_weights)\n",
    "            if \"freq\" in self.version:          # Get attention in frequency domain:\n",
    "                out_attn = torch.bmm(attn_weights, freq_vectors)\n",
    "            elif \"spatial\" in self.version:     # Get attention in spatial domain:\n",
    "                out_attn = torch.bmm(attn_weights, ifreq_vectors)\n",
    "                ### Check correct bmm:\n",
    "                # print(torch.eq(attn_outputs, out_attn))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Concat or add and linear\n",
    "            # print(\"Spatial vectors: \", spatial_vectors.shape)\n",
    "            # print(spatial_vectors)\n",
    "            # print(\"Output attention: \", out_attn.shape)\n",
    "            # print(out_attn)\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * out_attn)\n",
    "                # print(\"Out\", out)\n",
    "                embed = self.patch_to_embedding_add(out)                 # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * out_attn], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                 # Shape: (batchsize, num_patches, 2*patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "        else:   # Merge directly\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * freq_vectors)\n",
    "                embed = self.patch_to_embedding_add(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * freq_vectors], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "        print(\"Fusion out: \\n\", out)\n",
    "        print(\"Embed: \", embed)\n",
    "        # print(\"Embeded shape: \", embed.shape)\n",
    "\n",
    "        # Expand classify token to batchsize and add to patch embeddings:\n",
    "        cls_tokens = self.cls_token.expand(embed.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, embed), dim=1)   # (batchsize, num_patches+1, dim)\n",
    "        x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, in_dim, inner_dim=0, prj_out=False, qkv_embed=True, init_weight=True):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.qkv_embed = qkv_embed\n",
    "        self.init_weight = init_weight\n",
    "        self.to_out = nn.Identity()\n",
    "        if self.qkv_embed:\n",
    "            inner_dim = self.in_dim if inner_dim == 0 else inner_dim\n",
    "            self.to_k = nn.Linear(in_dim, inner_dim, bias=False)\n",
    "            self.to_v = nn.Linear(in_dim, inner_dim, bias = False)\n",
    "            self.to_q = nn.Linear(in_dim, inner_dim, bias = False)\n",
    "            self.to_out = nn.Sequential(\n",
    "                nn.Linear(inner_dim, in_dim),\n",
    "                nn.Dropout(p=0.1)\n",
    "            ) if prj_out else nn.Identity()\n",
    "\n",
    "        if self.init_weight:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        \"\"\"\n",
    "            x ~ rgb_vectors: (b, n, in_dim)\n",
    "            y ~ freq_vectors: (b, n, in_dim)\n",
    "            z ~ freq_vectors: (b, n, in_dim)\n",
    "            Returns:\n",
    "                attn_weight: (b, n, n)\n",
    "                attn_output: (b, n, in_dim)\n",
    "        \"\"\"\n",
    "        if self.qkv_embed:\n",
    "            q = self.to_q(x)\n",
    "            k = self.to_k(y)\n",
    "            v = self.to_v(z)\n",
    "        else:\n",
    "            q, k, v = x, y, z\n",
    "        print(\"x before scaledot: \", q)\n",
    "        print(\"y before scaledot: \", k)\n",
    "        print(\"z before scaledot: \", v)\n",
    "        out, attn = self.scale_dot(q, k, v, dropout_p=0.00)\n",
    "        out = self.to_out(out)\n",
    "        return out, attn\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        print(\"in scale dot.\")\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        print('q after: ', q)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        print(\"attn: \", attn)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "class MyDualEfficientViT(nn.Module):\n",
    "    def __init__(self, \\\n",
    "                image_size=224, num_classes=1, dim=1024,\\\n",
    "                depth=6, heads=8, mlp_dim=2048,\\\n",
    "                dim_head=64, dropout=0.15, emb_dropout=0.15,\\\n",
    "                backbone='xception_net', pretrained=True,\\\n",
    "                normalize_ifft=True,\\\n",
    "                flatten_type='patch',\\\n",
    "                conv_attn=False, ratio=5, qkv_embed=True, init_ca_weight=True, prj_out=False, inner_ca_dim=512, act='none',\\\n",
    "                patch_size=7, position_embed=False, pool='cls',\\\n",
    "                version='ca-fcat-0.5', unfreeze_blocks=-1, \\\n",
    "                init_linear=\"xavier\", init_layernorm=\"normal\", init_conv=\"kaiming\"):  \n",
    "        super(MyDualEfficientViT, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.features_size = {\n",
    "            'efficient_net': (1280, 4, 4),\n",
    "            'xception_net': (2048, 4, 4),\n",
    "        }\n",
    "        self.out_ext_channels = self.features_size[backbone][0]\n",
    "        \n",
    "        self.flatten_type = flatten_type # in ['patch', 'channel']\n",
    "        self.version = version  # in ['ca-rgb_cat-0.5', 'ca-freq_cat-0.5']\n",
    "        self.position_embed = position_embed\n",
    "        self.pool = pool\n",
    "        self.conv_attn = conv_attn\n",
    "        self.activation = self.get_activation(act)\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.rgb_extractor = self.get_feature_extractor(architecture=backbone, pretrained=pretrained, unfreeze_blocks=unfreeze_blocks, num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(architecture=backbone, pretrained=pretrained, unfreeze_blocks=unfreeze_blocks, num_classes=num_classes, in_channels=1)\n",
    "        self.normalize = nn.BatchNorm2d(num_features=self.out_ext_channels) if normalize_ifft else nn.Identity()\n",
    "        ############################# PATCH CONFIG ################################\n",
    "        \n",
    "        if self.flatten_type == 'patch':\n",
    "            # Kích thước của 1 patch\n",
    "            self.patch_size = patch_size\n",
    "            # Số lượng patches\n",
    "            self.num_patches = int((self.features_size[backbone][1] * self.features_size[backbone][2]) / (self.patch_size * self.patch_size))\n",
    "            # Patch_dim = P^2 * C\n",
    "            self.patch_dim = self.out_ext_channels//ratio * (self.patch_size ** 2)\n",
    "\n",
    "        ############################# CROSS ATTENTION #############################\n",
    "        if self.flatten_type == 'patch':\n",
    "            self.in_dim = self.patch_dim\n",
    "        else:\n",
    "            self.in_dim = int(self.features_size[backbone][1] * self.features_size[backbone][2])\n",
    "        if self.conv_attn:\n",
    "            self.query_conv = nn.Conv2d(in_channels=self.out_ext_channels, out_channels=self.out_ext_channels//ratio, kernel_size=1)\n",
    "            self.key_conv = nn.Conv2d(in_channels=self.out_ext_channels, out_channels=self.out_ext_channels//ratio, kernel_size=1)\n",
    "            self.value_conv = nn.Conv2d(in_channels=self.out_ext_channels, out_channels=self.out_ext_channels//ratio, kernel_size=1)\n",
    "\n",
    "        self.CA = CrossAttention(in_dim=self.in_dim, inner_dim=inner_ca_dim, prj_out=prj_out, qkv_embed=qkv_embed, init_weight=init_ca_weight)\n",
    "\n",
    "        ############################# VIT #########################################\n",
    "        # Number of vectors:\n",
    "        self.num_vecs = self.num_patches if self.flatten_type == 'patch' else self.out_ext_channels//ratio\n",
    "        # Embed vị trí cho từng vectors (nếu chia theo patch):\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_vecs+1, self.dim))\n",
    "        # Giảm chiều vector sau concat 2*patch_dim về D:\n",
    "        if 'cat' in self.version:\n",
    "            self.embedding = nn.Linear(2 * self.in_dim, self.dim)\n",
    "        else:\n",
    "            self.embedding = nn.Linear(self.in_dim, self.dim)\n",
    "\n",
    "        # Thêm 1 embedding vector cho classify token:\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init_linear, self.init_layernorm, self.init_conv = init_linear, init_layernorm, init_conv\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    def get_activation(self, act):\n",
    "        if act == 'relu':\n",
    "            activation = nn.ReLU(inplace=True)\n",
    "        elif act == 'tanh':\n",
    "            activation = nn.Tanh()\n",
    "        else:\n",
    "            activation = None\n",
    "        return activation\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", unfreeze_blocks=-1, pretrained=False, num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels, pretrained=bool(pretrained))\n",
    "            if unfreeze_blocks != -1:\n",
    "                # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "                for i in range(0, len(extractor._blocks)):\n",
    "                    for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "                        if i >= len(extractor._blocks) - unfreeze_blocks:\n",
    "                            param.requires_grad = True\n",
    "                        else:\n",
    "                            param.requires_grad = False\n",
    "        \n",
    "        if architecture == 'xception_net':\n",
    "            xception = get_model(\"xception\", pretrained=bool(pretrained))\n",
    "            extractor = nn.Sequential(*list(xception.children())[:-1])\n",
    "            extractor[0].final_block.pool = nn.Identity()\n",
    "            if in_channels != 3:\n",
    "                extractor[0].init_block.conv1.conv = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "\n",
    "            if unfreeze_blocks != -1:\n",
    "                blocks = len(extractor[0].children())\n",
    "                print(\"Number of blocks in xception: \", len(blocks))\n",
    "                for i, block in enumerate(extractor[0].children()):\n",
    "                    if i >= blocks - unfreeze_blocks:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    else:\n",
    "                        for param in block.parameters():\n",
    "                            param.requires_grad = False\n",
    "        print(\"Pretrained backbone: \", bool(pretrained))\n",
    "        # if not pretrained:\n",
    "        #     self.init_conv_weight(extractor)\n",
    "        return extractor\n",
    "\n",
    "    def flatten_to_vectors(self, feature):\n",
    "        vectors = None\n",
    "        if self.flatten_type == 'patch':\n",
    "            vectors = rearrange(feature, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "        elif self.flatten_type == 'channel':\n",
    "            vectors = rearrange(feature, 'b c h w -> b c (h w)')\n",
    "        else:\n",
    "            pass\n",
    "        return vectors\n",
    "\n",
    "    def ifft(self, freq_feature):\n",
    "        ifreq_feature = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_feature))) + 1e-10)  # Hơi ảo???\n",
    "        ifreq_feature = self.normalize(ifreq_feature)\n",
    "        return ifreq_feature\n",
    "\n",
    "    def fusion(self, rgb, out_attn):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            rgb --      b, n, d\n",
    "            out_attn -- b, n, d\n",
    "        \"\"\"\n",
    "        weight = float(self.version.split('-')[-1])\n",
    "        if 'cat' in self.version:\n",
    "            out = torch.cat([rgb, weight * out_attn], dim=2)\n",
    "        elif 'add' in self.version:\n",
    "            out = torch.add(rgb, weight * out_attn)\n",
    "        return out\n",
    "\n",
    "    def extract_feature(self, rgb_imgs, freq_imgs):\n",
    "        if self.backbone == 'efficient_net':\n",
    "            rgb_features = self.rgb_extractor.extract_features(rgb_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "            freq_features = self.freq_extractor.extract_features(freq_imgs)              # shape (batchsize, 1280, 4, 4)\n",
    "        else:\n",
    "            rgb_features = self.rgb_extractor(rgb_imgs)\n",
    "            freq_features = self.freq_extractor(freq_imgs)\n",
    "        return rgb_features, freq_features\n",
    "\n",
    "    def forward(self, rgb_imgs, freq_imgs):\n",
    "        rgb_features, freq_features = self.extract_feature(rgb_imgs, freq_imgs)\n",
    "        ifreq_features = self.ifft(freq_features)\n",
    "        # return rgb_features, freq_features, ifreq_features\n",
    "        # print(\"Features shape: \", rgb_features.shape, freq_features.shape, ifreq_features.shape)\n",
    "\n",
    "        # Turn to q, k, v if use conv-attention, and then flatten to vector:\n",
    "        if self.conv_attn:\n",
    "            rgb_query = self.query_conv(rgb_features)\n",
    "            freq_value = self.value_conv(freq_features)\n",
    "            ifreq_key = self.key_conv(ifreq_features)\n",
    "            ifreq_value = self.value_conv(ifreq_features)\n",
    "        else:\n",
    "            rgb_query = rgb_features\n",
    "            freq_value = freq_features\n",
    "            ifreq_key = ifreq_features\n",
    "            ifreq_value = ifreq_features\n",
    "        # print(\"Q K V shape: \", rgb_query.shape, freq_value.shape, ifreq_key.shape, ifreq_value.shape)\n",
    "        rgb_query_vectors = self.flatten_to_vectors(rgb_query)\n",
    "        freq_value_vectors = self.flatten_to_vectors(freq_value)\n",
    "        ifreq_key_vectors = self.flatten_to_vectors(ifreq_key)\n",
    "        ifreq_value_vectors = self.flatten_to_vectors(ifreq_value)\n",
    "        # print(\"Vectors shape: \", rgb_query_vectors.shape, freq_value_vectors.shape, ifreq_key_vectors.shape, ifreq_value_vectors.shape)\n",
    "\n",
    "        ##### Cross attention and fusion:\n",
    "        # print(\"1: \", rgb_query_vectors)\n",
    "        # print(\"2: \", ifreq_key_vectors)\n",
    "        # print(\"3: \", ifreq_value_vectors)\n",
    "        out, attn_weight = self.CA(rgb_query_vectors, ifreq_key_vectors, ifreq_value_vectors)\n",
    "\n",
    "        attn_out = torch.bmm(attn_weight, freq_value_vectors)\n",
    "        fusion_out = self.fusion(rgb_query_vectors, attn_out)\n",
    "        if self.activation is not None:\n",
    "            fusion_out = self.activation(fusion_out)\n",
    "        # print(\"Fusion shape: \", fusion_out.shape)\n",
    "        embed = self.embedding(fusion_out)\n",
    "        print(\"Fusion: \\n\", fusion_out)\n",
    "        print(\"embed: \", embed)\n",
    "        # print(\"Inner ViT shape: \", embed.shape)\n",
    "\n",
    "        ##### Forward to ViT\n",
    "        # Expand classify token to batchsize and add to patch embeddings:\n",
    "        cls_tokens = self.cls_token.expand(embed.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, embed), dim=1)   # (batchsize, in_dim+1, dim)\n",
    "        if self.position_embed:\n",
    "            x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x.mean(dim = 1) if self.pool == 'mean' else x[:, 0])\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "from torchsummary import summary\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(0)\n",
    "    x = torch.ones(1, 3, 128, 128)\n",
    "    y = torch.ones(1, 1, 128, 128)\n",
    "    model_1 = MyDualEfficientViT( image_size=128, num_classes=1, dim=1024,\\\n",
    "                                depth=6, heads=8, mlp_dim=2048,\\\n",
    "                                dim_head=64, dropout=0.15, emb_dropout=0.15,\\\n",
    "                                backbone='xception_net', pretrained=True,\\\n",
    "                                normalize_ifft=False,\\\n",
    "                                flatten_type='patch',\\\n",
    "                                conv_attn=False, ratio=1, qkv_embed=False, inner_ca_dim=0, init_ca_weight=False, prj_out=False, act='none',\\\n",
    "                                patch_size=1, position_embed=True, pool='cls',\\\n",
    "                                version='ca-fadd-0.8', unfreeze_blocks=-1)\n",
    "    model_1.eval()\n",
    "    with torch.no_grad():\n",
    "        print(\"Model 1\")\n",
    "        rgb_1, freq_1, ifreq_1 = model_1(x, y)\n",
    "        print(\"************************************* RGB 1 ***\")\n",
    "        # print(freq_1)\n",
    "        import numpy as np\n",
    "\n",
    "\n",
    "    model_2 = OriDualEfficientViT(channels=2048, image_size=128, num_classes=1, dim=1024,\\\n",
    "                                depth=6, heads=8, mlp_dim=2048,\\\n",
    "                                dim_head=64, dropout=0.15, emb_dropout=0.15,\\\n",
    "                                patch_size=1, backbone=\"xception_net\", version='cross_attention-freq-add', weight=0.8, freeze=0)\n",
    "    model_2.eval()\n",
    "    with torch.no_grad():\n",
    "        print(\"Model 2\")\n",
    "        rgb_2, freq_2, ifreq_2 = model_2(x, y)\n",
    "        print(\"************************************* RGB 2 ***\")\n",
    "        print(freq_2)\n",
    "\n",
    "    if torch.equal(rgb_1, rgb_2):\n",
    "        print(\"Equal rgb\")\n",
    "    if torch.equal(freq_1, freq_2):\n",
    "        print(\"Equal freq\")\n",
    "    if torch.equal(ifreq_1, ifreq_2):\n",
    "        print(\"Equal ifreq\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73e4523d5c5fcabc881bfbabdc03d28b885253c65d62f8f3eb31939c7679911f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('phucnp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
